<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>nktmemo_ja</title>
    <description>nktmemo provides interesting articles about Machine Learning and NLP. 
</description>
    <link>http://nktmemoja.github.io/</link>
    <atom:link href="http://nktmemoja.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 26 May 2016 02:04:28 +0900</pubDate>
    <lastBuildDate>Thu, 26 May 2016 02:04:28 +0900</lastBuildDate>
    <generator>Jekyll v3.0.0</generator>
    
      <item>
        <title>Adjusted Rand Index (ARI) について勉強してみた</title>
        <description>&lt;p&gt;クラスタリングの性能評価でよくAdjusted Rand Index (ARI) が使われます．
僕もよく使っていますが，お恥ずかしながらよく知らずに使っていました．
というのも，Rand Index (RI) は理解できるのですが，ARIのAdjustedの意味がよくわかりませんでした．
というわけで，勉強してみました．&lt;/p&gt;

&lt;p&gt;まず，クラスタリングは，サンプル&lt;script type=&quot;math/tex&quot;&gt;S = \{o_i\}_{i=1}^n&lt;/script&gt;を&lt;script type=&quot;math/tex&quot;&gt;k (1 \leq k \leq n)&lt;/script&gt;個の互いに素なグループに分けることを目的とします．
この記事では，真のクラスタリング結果と，クラスタリングアルゴリズムの結果が似ているかどうかを測ることを目的とします．
なお，ここでは，クラスタリングの結果をクラスタラベルで表現することにします．&lt;/p&gt;

&lt;p&gt;さて，&lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt;に対する，
真のクラスタラベルを&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{\ell}=[\ell_1 \cdots \ell_n], \ell_i \in \{1, \ldots, k\} \forall i=1, \ldots, n&lt;/script&gt;，
クラスタリングアルゴリズムによるクラスタラベルを&lt;script type=&quot;math/tex&quot;&gt;\hat{ \boldsymbol{\ell}}=[\hat{\ell}_1 \cdots \hat{\ell}_n], \hat{\ell}_i \in \{1, \ldots, \hat{k}\} \forall i=1, \ldots, n&lt;/script&gt;とします．
クラスタリングの結果は，分類のようにラベルで考えるのではなく，ラベルのペアで考えます．
つまり，「このサンプルとこのサンプルは同じクラスタに属するか否か」で性能を評価します．&lt;/p&gt;

&lt;p&gt;そのような指標の一つである，Rand Indexは次式で定義されます（自己流です）．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
RI(\boldsymbol{\ell}, \hat{\boldsymbol{\ell}}) = \frac{1}{|C(n, 2)|}\sum_{(i, j) \in C(n, 2)} I(I(\ell_i = \ell_j) = I(\hat{\ell}_i = \hat{\ell}_j))
\end{align*}&lt;/script&gt;

&lt;p&gt;ただし，
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
C(n, 2) = \{(i, j) | i, j \in \{1, \ldots, n\}, i&lt;j \} %]]&gt;&lt;/script&gt;
はサンプルの2つの全組み合わせ．&lt;/p&gt;

&lt;p&gt;これはわかりやすいです．サンプルペアが同じクラスタに属するのか違うクラスタに属するのかが測れそうです．&lt;/p&gt;

&lt;p&gt;よくRIの欠点として，「二つのクラスタリングに相関がなくても，高い値をとってしまう」という説明がなされます．
これの意味は，「適当（ランダム）にやっても高い値をとってしまう」ということではないかと思います．
さて，この「相関」ですが，普通「相関」というと確率変数間の相関をイメージしますよね．
ということでどの確率変数とどの確率変数の間の相関のことを言っているのかから考えていきます．&lt;/p&gt;

&lt;p&gt;まず，以下の二つの標本を考えます：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
X &amp;= \{I(\ell_i = \ell_j) | (i,j) \in C(n, 2)\} \\
Y &amp;= \{I(\hat{\ell}_i = \hat{\ell}_j) | (i,j) \in C(n, 2)\}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;そして，&lt;script type=&quot;math/tex&quot;&gt;X, Y&lt;/script&gt;に対応する確率変数をそれぞれ&lt;script type=&quot;math/tex&quot;&gt;x, y&lt;/script&gt;とし，&lt;script type=&quot;math/tex&quot;&gt;X, Y&lt;/script&gt;はそれぞれ&lt;script type=&quot;math/tex&quot;&gt;x, y&lt;/script&gt;の従う確率分布から独立に生成されたと仮定します (i.i.d.)．
上で言った「相関」とは，多分この&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;と&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;の相関のことだと思います．&lt;/p&gt;

&lt;p&gt;さて，適当にやった時，つまり，&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;と&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;に相関がない (&lt;script type=&quot;math/tex&quot;&gt;x \perp y&lt;/script&gt;) 時に，RIの値を下げたいのですが，どうしたら良いでしょうか？
もう少し具体的にいうと，RIの値は分子で決まるので，RIの分子の値を下げたいのですが，どうしたら良いでしょう？
直感的にいうと，「適当にクラスタリングしたら，RIの分子がどのくらいになるか」がわかれば，それをペナルティに設定すれば良さそうではないですか？
というわけで，ペナルティ = 適当にクラスタリングした時のRIの分子のおおよその値 = &lt;script type=&quot;math/tex&quot;&gt;x \perp y&lt;/script&gt;の時のRIの分子のおおよその値を求めていきます．&lt;/p&gt;

&lt;p&gt;とりあえず，見やすくするために，上の&lt;script type=&quot;math/tex&quot;&gt;X, Y&lt;/script&gt;を，
&lt;script type=&quot;math/tex&quot;&gt;X = \{x_i\}_{i=1}^{m}, Y = \{y_i\}_{i=1}^{m}, m={}_n C _2&lt;/script&gt;と置き直します．
さらに，&lt;script type=&quot;math/tex&quot;&gt;Z = \{z_i\}_{i=1}^m, z_i = I(x_i = y_i)&lt;/script&gt;と置きます．
そうすると，RIは以下の式になります．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
RI(y, \hat{y}) = \frac{1}{m} \sum_{i=1}^m I(x_i = y_i)
\end{align*}&lt;/script&gt;

&lt;p&gt;上でざっくり定義したペナルティを求めるために，&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;と&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;が，それぞれベルヌーイ分布に従う，
つまり，&lt;script type=&quot;math/tex&quot;&gt;x \sim Bernoulli(p_x), y \sim Bernoulli(p_y), x \perp y&lt;/script&gt;を仮定します．
さらに，&lt;script type=&quot;math/tex&quot;&gt;Bernoulli(p_x), Bernoulli(p_y)&lt;/script&gt;の確率質量関数をそれぞれ&lt;script type=&quot;math/tex&quot;&gt;p(x), p(y)&lt;/script&gt;とします．
ここで，RIの分子の和の中に出現する，以下で定義される確率変数&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;を考えます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
z = I(x = y)
\end{align*}&lt;/script&gt;

&lt;p&gt;さらに，&lt;script type=&quot;math/tex&quot;&gt;Z = \{z_i\}_{i=1}^m, z_i = I(x_i = y_i)&lt;/script&gt;と置きます．
そうすると，RIの式が，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
RI(y, \hat{y}) = \frac{1}{m} \sum_{i=1}^m z_i
\end{align*}&lt;/script&gt;

&lt;p&gt;と超絶シンプルになりました．
さて，上で定義したペナルティは「&lt;script type=&quot;math/tex&quot;&gt;x \perp y&lt;/script&gt;の時のRIの分子のおおよその値」でした．
ここで，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
z&#39; = \sum_{i=1}^m z_i
\end{align*}&lt;/script&gt;

&lt;p&gt;と定義すると，求めたいペナルティはまさに，「&lt;script type=&quot;math/tex&quot;&gt;x \perp y&lt;/script&gt;の時の&lt;script type=&quot;math/tex&quot;&gt;z&#39;&lt;/script&gt;の期待値」に相当するのではないでしょうか？
というわけでここから，「&lt;script type=&quot;math/tex&quot;&gt;x \perp y&lt;/script&gt;の時の&lt;script type=&quot;math/tex&quot;&gt;z&#39;&lt;/script&gt;の期待値」を求めていきます．&lt;/p&gt;

&lt;p&gt;まず，&lt;script type=&quot;math/tex&quot;&gt;x \perp y&lt;/script&gt;より，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(z=1) &amp;= p(x=1, y=1) + p(x=0, y=0) = p(x=1)p(y=1) + p(x=0)p(y=0)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;となります．よって，&lt;script type=&quot;math/tex&quot;&gt;z&#39;&lt;/script&gt;の期待値は，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathbb{E}\left[z&#39;\right] &amp;= \mathbb{E}\left[\sum_{i=1}^m z_i\right] \\
&amp;= \sum_{i=1}^m \mathbb{E}\left[z_i\right] \\
&amp;= \sum_{i=1}^m 0 \cdot p(z=0) + 1 \cdot p(z=1) \\
&amp;= m p(z=1) \\
&amp;= m \left(p(x=1)p(y=1) + p(x=0)p(y=0) \right)\\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;と書けます．すっきりしましたね．
&lt;script type=&quot;math/tex&quot;&gt;p(x=0), p(x=1), p(y=0), p(y=1)&lt;/script&gt;は未知なので，標本から推定してやらねばなりません．
ここでは，以下のように，最尤推定で求めることにしましょう．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(x=1) &amp;= \frac{1}{m} \sum_{i=1}^m x_i, \ p(x=0) = 1 - p(x=1) \\
p(y=1) &amp;= \frac{1}{m} \sum_{i=1}^m y_i, \ p(y=0) = 1 - p(y=1)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;あとは&lt;script type=&quot;math/tex&quot;&gt;z&#39;&lt;/script&gt;の期待値をRIの分母分子から引いてやるとARIに一致します．
Wikipediaの定義式なんかと一致するのを示したほうが良かったですね…．
時間があれば追記しておきます．
なお，具体例は以下のページが参考になります．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://y-uti.hatenablog.jp/entry/2014/01/19/133936&quot; target=&quot;_blank&quot;&gt;http://y-uti.hatenablog.jp/entry/2014/01/19/133936&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 24 May 2016 19:46:44 +0900</pubDate>
        <link>http://nktmemoja.github.io/jekyll/update/2016/05/24/ari.html</link>
        <guid isPermaLink="true">http://nktmemoja.github.io/jekyll/update/2016/05/24/ari.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>Adjusted Rand Index (ARI) について勉強してみた</title>
        <description>&lt;p&gt;クラスタリングの性能評価でよくAdjusted Rand Index (ARI) が使われます．
僕もよく使っていますが，お恥ずかしながらよく知らずに使っていました．
というのも，Rand Index (RI) は理解できるのですが，ARIのAdjustedの意味がよくわかりませんでした．
というわけで，勉強してみました．&lt;/p&gt;

&lt;p&gt;まず，クラスタリングは，サンプル&lt;script type=&quot;math/tex&quot;&gt;S = \{o_i\}_{i=1}^n&lt;/script&gt;を&lt;script type=&quot;math/tex&quot;&gt;k (1 \leq k \leq n)&lt;/script&gt;個の互いに素なグループに分けることを目的とします．
この記事では，真のクラスタリング結果と，クラスタリングアルゴリズムの結果が似ているかどうかを測ることを目的とします．
なお，ここでは，クラスタリングの結果をクラスタラベルで表現することにします．&lt;/p&gt;

&lt;p&gt;さて，&lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt;に対する，
真のクラスタラベルを&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{\ell}=[\ell_1 \cdots \ell_n], \ell_i \in \{1, \ldots, k\} \forall i=1, \ldots, n&lt;/script&gt;，
クラスタリングアルゴリズムによるクラスタラベルを&lt;script type=&quot;math/tex&quot;&gt;\hat{ \boldsymbol{\ell}}=[\hat{\ell}_1 \cdots \hat{\ell}_n], \hat{\ell}_i \in \{1, \ldots, \hat{k}\} \forall i=1, \ldots, n&lt;/script&gt;とします．
クラスタリングの結果は，分類のようにラベルで考えるのではなく，ラベルのペアで考えます．
つまり，「このサンプルとこのサンプルは同じクラスタに属するか否か」で性能を評価します．&lt;/p&gt;

&lt;p&gt;そのような指標の一つである，Rand Indexは次式で定義されます（自己流です）．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
RI(\boldsymbol{\ell}, \hat{\boldsymbol{\ell}}) = \frac{1}{|C(n, 2)|}\sum_{(i, j) \in C(n, 2)} I(I(\ell_i = \ell_j) = I(\hat{\ell}_i = \hat{\ell}_j))
\end{align*}&lt;/script&gt;

&lt;p&gt;ただし，
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
C(n, 2) = \{(i, j) | i, j \in \{1, \ldots, n\}, i&lt;j \} %]]&gt;&lt;/script&gt;
はサンプルの2つの全組み合わせ．&lt;/p&gt;

&lt;p&gt;これはわかりやすいです．サンプルペアが同じクラスタに属するのか違うクラスタに属するのかが測れそうです．&lt;/p&gt;

&lt;p&gt;よくRIの欠点として，「二つのクラスタリングに相関がなくても，高い値をとってしまう」という説明がなされます．
これの意味は，「適当（ランダム）にやっても高い値をとってしまう」ということではないかと思います．
さて，この「相関」ですが，普通「相関」というと確率変数間の相関をイメージしますよね．
ということでどの確率変数とどの確率変数の間の相関のことを言っているのかから考えていきます．&lt;/p&gt;

&lt;p&gt;まず，以下の二つの標本を考えます：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
X &amp;= \{I(\ell_i = \ell_j) | (i,j) \in C(n, 2)\} \\
Y &amp;= \{I(\hat{\ell}_i = \hat{\ell}_j) | (i,j) \in C(n, 2)\}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;そして，&lt;script type=&quot;math/tex&quot;&gt;X, Y&lt;/script&gt;に対応する確率変数をそれぞれ&lt;script type=&quot;math/tex&quot;&gt;x, y&lt;/script&gt;とし，&lt;script type=&quot;math/tex&quot;&gt;X, Y&lt;/script&gt;はそれぞれ&lt;script type=&quot;math/tex&quot;&gt;x, y&lt;/script&gt;の従う確率分布から独立に生成されたと仮定します (i.i.d.)．
上で言った「相関」とは，多分この&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;と&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;の相関のことだと思います．&lt;/p&gt;

&lt;p&gt;さて，適当にやった時，つまり，&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;と&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;に相関がない (&lt;script type=&quot;math/tex&quot;&gt;x \perp y&lt;/script&gt;) 時に，RIの値を下げたいのですが，どうしたら良いでしょうか？
もう少し具体的にいうと，RIの値は分子で決まるので，RIの分子の値を下げたいのですが，どうしたら良いでしょう？
直感的にいうと，「適当にクラスタリングしたら，RIの分子がどのくらいになるか」がわかれば，それをペナルティに設定すれば良さそうではないですか？
というわけで，ペナルティ = 適当にクラスタリングした時のRIの分子のおおよその値 = &lt;script type=&quot;math/tex&quot;&gt;x \perp y&lt;/script&gt;の時のRIの分子のおおよその値を求めていきます．&lt;/p&gt;

&lt;p&gt;とりあえず，見やすくするために，上の&lt;script type=&quot;math/tex&quot;&gt;X, Y&lt;/script&gt;を，
&lt;script type=&quot;math/tex&quot;&gt;X = \{x_i\}_{i=1}^{m}, Y = \{y_i\}_{i=1}^{m}, m={}_n C _2&lt;/script&gt;と置き直します．
さらに，&lt;script type=&quot;math/tex&quot;&gt;Z = \{z_i\}_{i=1}^m, z_i = I(x_i = y_i)&lt;/script&gt;と置きます．
そうすると，RIは以下の式になります．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
RI(y, \hat{y}) = \frac{1}{m} \sum_{i=1}^m I(x_i = y_i)
\end{align*}&lt;/script&gt;

&lt;p&gt;上でざっくり定義したペナルティを求めるために，&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;と&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;が，それぞれベルヌーイ分布に従う，
つまり，&lt;script type=&quot;math/tex&quot;&gt;x \sim Bernoulli(p_x), y \sim Bernoulli(p_y), x \perp y&lt;/script&gt;を仮定します．
さらに，&lt;script type=&quot;math/tex&quot;&gt;Bernoulli(p_x), Bernoulli(p_y)&lt;/script&gt;の確率質量関数をそれぞれ&lt;script type=&quot;math/tex&quot;&gt;p(x), p(y)&lt;/script&gt;とします．
ここで，RIの分子の和の中に出現する，以下で定義される確率変数&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;を考えます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
z = I(x = y)
\end{align*}&lt;/script&gt;

&lt;p&gt;さらに，&lt;script type=&quot;math/tex&quot;&gt;Z = \{z_i\}_{i=1}^m, z_i = I(x_i = y_i)&lt;/script&gt;と置きます．
そうすると，RIの式が，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
RI(y, \hat{y}) = \frac{1}{m} \sum_{i=1}^m z_i
\end{align*}&lt;/script&gt;

&lt;p&gt;と超絶シンプルになりました．
さて，上で定義したペナルティは「&lt;script type=&quot;math/tex&quot;&gt;x \perp y&lt;/script&gt;の時のRIの分子のおおよその値」でした．
ここで，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
z&#39; = \sum_{i=1}^m z_i
\end{align*}&lt;/script&gt;

&lt;p&gt;と定義すると，求めたいペナルティはまさに，「&lt;script type=&quot;math/tex&quot;&gt;x \perp y&lt;/script&gt;の時の&lt;script type=&quot;math/tex&quot;&gt;z&#39;&lt;/script&gt;の期待値」に相当するのではないでしょうか？
というわけでここから，「&lt;script type=&quot;math/tex&quot;&gt;x \perp y&lt;/script&gt;の時の&lt;script type=&quot;math/tex&quot;&gt;z&#39;&lt;/script&gt;の期待値」を求めていきます．&lt;/p&gt;

&lt;p&gt;まず，&lt;script type=&quot;math/tex&quot;&gt;x \perp y&lt;/script&gt;より，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(z=1) &amp;= p(x=1, y=1) + p(x=0, y=0) = p(x=1)p(y=1) + p(x=0)p(y=0)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;となります．よって，&lt;script type=&quot;math/tex&quot;&gt;z&#39;&lt;/script&gt;の期待値は，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathbb{E}\left[z&#39;\right] &amp;= \mathbb{E}\left[\sum_{i=1}^m z_i\right] \\
&amp;= \sum_{i=1}^m \mathbb{E}\left[z_i\right] \\
&amp;= \sum_{i=1}^m 0 \cdot p(z=0) + 1 \cdot p(z=1) \\
&amp;= m p(z=1) \\
&amp;= m \left(p(x=1)p(y=1) + p(x=0)p(y=0) \right)\\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;と書けます．すっきりしましたね．
&lt;script type=&quot;math/tex&quot;&gt;p(x=0), p(x=1), p(y=0), p(y=1)&lt;/script&gt;は未知なので，標本から推定してやらねばなりません．
ここでは，以下のように，最尤推定で求めることにしましょう．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(x=1) &amp;= \frac{1}{m} \sum_{i=1}^m x_i, \ p(x=0) = 1 - p(x=1) \\
p(y=1) &amp;= \frac{1}{m} \sum_{i=1}^m y_i, \ p(y=0) = 1 - p(y=1)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;あとは&lt;script type=&quot;math/tex&quot;&gt;z&#39;&lt;/script&gt;の期待値をRIの分母分子から引いてやるとARIに一致します．
Wikipediaの定義式なんかと一致するのを示したほうが良かったですね…．
時間があれば追記しておきます．
なお，具体例は以下のページが参考になります．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://y-uti.hatenablog.jp/entry/2014/01/19/133936&quot; target=&quot;_blank&quot;&gt;http://y-uti.hatenablog.jp/entry/2014/01/19/133936&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 24 May 2016 19:46:44 +0900</pubDate>
        <link>http://nktmemoja.github.io/jekyll/update/2016/05/24/ari.html</link>
        <guid isPermaLink="true">http://nktmemoja.github.io/jekyll/update/2016/05/24/ari.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>pystructのChainCRF + FrankWolfSVMで各系列への予測値を取得する方法</title>
        <description>&lt;p&gt;pystructは構造化学習のいくつかの手法を，Python上に実装したライブラリである．
その中に系列ラベリングの手法である，条件付き確率場も実装されている．
使い方は簡単で，例えば&lt;a href=&quot;https://pystruct.github.io/auto_examples/plot_letters.html#sphx-glr-auto-examples-plot-letters-py&quot;&gt;OCR Letter sequence recognition&lt;/a&gt;にデモコードがある．&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;ypred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_joint_feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ypred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

</description>
        <pubDate>Mon, 29 Feb 2016 17:39:04 +0900</pubDate>
        <link>http://nktmemoja.github.io/jekyll/update/2016/02/29/pystruct-crf.html</link>
        <guid isPermaLink="true">http://nktmemoja.github.io/jekyll/update/2016/02/29/pystruct-crf.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>単語間関係分類</title>
        <description>&lt;h2 id=&quot;section&quot;&gt;定式化&lt;/h2&gt;

&lt;p&gt;定義：&lt;br /&gt;
　&lt;script type=&quot;math/tex&quot;&gt;\mathcal{X}&lt;/script&gt;: 単語空間&lt;br /&gt;
　&lt;script type=&quot;math/tex&quot;&gt;\mathcal{Z} = \{(x, y)|x, y \in \mathcal{X}\}&lt;/script&gt;: 単語間関係空間&lt;br /&gt;
　&lt;script type=&quot;math/tex&quot;&gt;\mathcal{S} = \{-1, 1\}&lt;/script&gt;: 関係候補成立のラベル&lt;br /&gt;
　&lt;script type=&quot;math/tex&quot;&gt;\mathcal{T} = \{-1, 1\}&lt;/script&gt;: 関係成立のラベル&lt;/p&gt;

&lt;p&gt;目的:
&lt;script type=&quot;math/tex&quot;&gt;p(s,t|x,y), \ ((x,y) \in \mathcal{Z}, s \in \mathcal{S}, t \in \mathcal{T})&lt;/script&gt;
の推定&lt;br /&gt;
仮定: &lt;script type=&quot;math/tex&quot;&gt;p(s,t|x,y) = p(s|x,y)p(t|x,y)&lt;/script&gt;  つまり，&lt;script type=&quot;math/tex&quot;&gt;x,y&lt;/script&gt;が関係分類の候補であることと，&lt;script type=&quot;math/tex&quot;&gt;x,y&lt;/script&gt;の関係が成立していることは独立である．&lt;br /&gt;
制約: &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\forall x,y \in \mathcal{X}, \ p(t=1|x,y) &gt; p(t=-1|x,y) \Leftrightarrow p(t=1|y,x) &lt; p(t=-1|y,x) %]]&gt;&lt;/script&gt;&lt;br /&gt;
この制約は，&lt;script type=&quot;math/tex&quot;&gt;p(t=1|y,x) = p(t=-1|x,y)&lt;/script&gt;ならば満たされるので，これを制約とする．&lt;br /&gt;
設定: トレーニングデータとして，&lt;script type=&quot;math/tex&quot;&gt;\{(x_i, y_i, t_i)\}_{i=1}^n&lt;/script&gt;が与えられる&lt;br /&gt;
補足: &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;を考えるのは，&lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;だけだと，関係が成立するかしないかしか表現できないので，
どっちでもない場合というか2語が全く関係ない場合にめちゃくちゃな予測をしてしまう．
これは，分類問題一般に言えることだが，定義したクラスに対して，「それ以外」な入力が必ずと言っていいほど存在するので，
本来はこのような設定で問題を考えるべきだと僕は思う．
このような設定の場合は，One Class SVMなどのOutlier Detectionの手法が用いられる．
予測:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;(\boldsymbol{x}, \boldsymbol{y})&lt;/script&gt;の関係が成り立っているかどうかは以下のように予測する．
まず，以下で定義する&lt;script type=&quot;math/tex&quot;&gt;(\hat{s}, \hat{t})&lt;/script&gt;を求める．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
(\hat{s}, \hat{t}) = \text{argmax}_{(s,t) \in \mathcal{S} \times \mathcal{T}} \ p(s, t|\boldsymbol{x},\boldsymbol{y})
\end{align*}&lt;/script&gt;

&lt;p&gt;そして，&lt;script type=&quot;math/tex&quot;&gt;\hat{s}=1, \hat{t}=1&lt;/script&gt;の時に限り，&lt;script type=&quot;math/tex&quot;&gt;(\boldsymbol{x}, \boldsymbol{y})&lt;/script&gt;の関係が成り立っているとする．&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;p(s|\boldsymbol{x},\boldsymbol{y})&lt;/script&gt;について．
今回は，関係候補となる単語同士は互いに似ているとして，類似度&lt;script type=&quot;math/tex&quot;&gt;s: \mathcal{X} \times \mathcal{X} \rightarrow [0,1]&lt;/script&gt;を用いて，
&lt;script type=&quot;math/tex&quot;&gt;p(s=1|x,y) = s(x,y)&lt;/script&gt;
と定義する．
さらに，簡単のため，&lt;script type=&quot;math/tex&quot;&gt;\mathcal{X}=\mathbb{R}^d&lt;/script&gt;とする．&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;p(t|\boldsymbol{x},\boldsymbol{y})&lt;/script&gt;
の推定には，ロジスティック回帰を用いる．
&lt;script type=&quot;math/tex&quot;&gt;p(t|\boldsymbol{x},\boldsymbol{y})&lt;/script&gt;
を以下で定義される，対数線形モデル&lt;script type=&quot;math/tex&quot;&gt;q(t|\boldsymbol{x},\boldsymbol{y};\boldsymbol{\theta}), \boldsymbol{\theta} \in \mathbb{R}^d&lt;/script&gt;でモデル化する．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
q(t|\boldsymbol{x},\boldsymbol{y};\boldsymbol{\theta}) = \frac{1}{1 + \exp\left(t \cdot \boldsymbol{\theta}^T(\boldsymbol{x}-\boldsymbol{y})\right)}
\end{align*}&lt;/script&gt;

&lt;p&gt;このモデルでは，&lt;script type=&quot;math/tex&quot;&gt;(\boldsymbol{x}, \boldsymbol{y})&lt;/script&gt;を&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{x}-\boldsymbol{y}&lt;/script&gt;として扱っている．
このようにモデルを定義すると，&lt;script type=&quot;math/tex&quot;&gt;q(t=1|\boldsymbol{y},\boldsymbol{x}) = q(t=-1|\boldsymbol{x},\boldsymbol{y})&lt;/script&gt;となり，自動的に制約が満たされる．&lt;/p&gt;

&lt;p&gt;以上より，解くべき問題は，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\max_{\boldsymbol{\theta}} &amp;\ \sum_{i=1}^n \log q(t_i|\boldsymbol{x}_i,\boldsymbol{y}_i;\boldsymbol{\theta}) \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;となり，通常のロジスティック回帰に一致する．&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;実験&lt;/h2&gt;

&lt;h3 id=&quot;word2vec&quot;&gt;Word2Vec&lt;/h3&gt;

&lt;p&gt;gensim.Word2Vec, d=100, 他default&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;関係学習&lt;/h3&gt;

&lt;h3 id=&quot;section-3&quot;&gt;結果&lt;/h3&gt;
</description>
        <pubDate>Sun, 21 Feb 2016 04:06:36 +0900</pubDate>
        <link>http://nktmemoja.github.io/jekyll/update/2016/02/21/word-relation-learning.html</link>
        <guid isPermaLink="true">http://nktmemoja.github.io/jekyll/update/2016/02/21/word-relation-learning.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>劣モジュラ最適化による文書要約（理論編）</title>
        <description>&lt;p&gt;&lt;a rel=&quot;nofollow&quot; href=&quot;http://www.amazon.co.jp/gp/product/4061529099/ref=as_li_ss_tl?ie=UTF8&amp;amp;camp=247&amp;amp;creative=7399&amp;amp;creativeASIN=4061529099&amp;amp;linkCode=as2&amp;amp;tag=nettodesyuu00-22&quot; target=&quot;_blank&quot;&gt;劣モジュラ最適化と機械学習 (機械学習プロフェッショナルシリーズ)&lt;/a&gt;&lt;img src=&quot;http://ir-jp.amazon-adsystem.com/e/ir?t=nettodesyuu00-22&amp;amp;l=as2&amp;amp;o=9&amp;amp;a=4061529099&quot; width=&quot;1&quot; height=&quot;1&quot; border=&quot;0&quot; alt=&quot;&quot; style=&quot;border:none !important; margin:0px !important;&quot; /&gt;
を読んでいます．
この中に，劣モジュラ最適化の文書要約への応用が紹介されていたので，勉強がてら書こうと思います．
やっぱりこういう理論的なものを学ぶ時は，応用を同時に学ぶといいですよね．&lt;/p&gt;

&lt;p&gt;さて，まずは文書要約について．
文書要約手法には，抽出型と生成型があるらしいです．
抽出型は，要約対象に出てくる要素（例えば文）を組み合わせて，要約を作るアプローチです．
生成型は，要約対象の文書の内容を理解して，要約を生成するアプローチです．
直感的には，抽出型の方がシンプルで簡単そうな気がします．
というわけで，今回は抽出型について書こうと思います．&lt;/p&gt;

&lt;p&gt;さて，問題を定式化していきましょう．
抽出対象は文にしようと思います．
文書を文の集合と考えると，ある文書は&lt;script type=&quot;math/tex&quot;&gt;V=\{1,\ldots,n\}&lt;/script&gt;と表されます．
ここで，&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;はその文書が含む総文数です．
抽出型の文書要約はなんらかの基準で&lt;script type=&quot;math/tex&quot;&gt;S \subseteq V&lt;/script&gt;を選ぶ問題だと解釈できます．&lt;/p&gt;

&lt;p&gt;このような基準は，自然言語処理分野で数多く提案されているみたいですが，
今回は本に出てきた，「関連性」と「多様性」による基準を採用しようと思います．
ここでいう関連性とは，要約文と本文との関連度を表し，
多様性は，要約文に含まれる文の多様性を表します．
つまり，この二つを同時に最大化したいわけです．
さて，要約文&lt;script type=&quot;math/tex&quot;&gt;S \subseteq V&lt;/script&gt;の関連性を&lt;script type=&quot;math/tex&quot;&gt;L(S)&lt;/script&gt;，多様性を&lt;script type=&quot;math/tex&quot;&gt;R(S)&lt;/script&gt;と表し，要約文の良さを表す関数&lt;script type=&quot;math/tex&quot;&gt;f_{doc}&lt;/script&gt;を以下で定義します．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
f_{doc} = L(S) + \lambda R(S)
\end{align*}&lt;/script&gt;

&lt;p&gt;ここで，&lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;はトレードオフを調節するパラメータです．
さて，まずは全文&lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt;と要約文&lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt;の関連性を表す関数&lt;script type=&quot;math/tex&quot;&gt;L(S)&lt;/script&gt;について考えましょう．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
L(S) = \sum_{i \in V} \min\{C_i(S), \gamma C_i(V)\}
\end{align*}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
C_i(S) = \sum_{j \in S} s_{ij}
\end{align*}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
R(S) = \sum_{k=1}^K \sqrt{\sum_{j \in P_k \cap S} r_j}
\end{align*}&lt;/script&gt;
</description>
        <pubDate>Tue, 09 Feb 2016 00:09:34 +0900</pubDate>
        <link>http://nktmemoja.github.io/jekyll/update/2016/02/09/document-summarization-via-submodular.html</link>
        <guid isPermaLink="true">http://nktmemoja.github.io/jekyll/update/2016/02/09/document-summarization-via-submodular.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>ソフトマージンSVMのヒンジ損失最小化学習としての解釈とその実装</title>
        <description>&lt;p&gt;前回，&lt;a href=&quot;http://nktmemoja.github.io/jekyll/update/2016/01/07/hardmarginsvmformulation.html&quot;&gt;ハードマージンSVMの定式化&lt;/a&gt;について書いたので，
今回はソフトマージンSVMの定式化をしようと思います．
さらに，それをヒンジ損失最小化学習として解釈できることを示し，
最後に確率的勾配法を使って実装したいと思います．&lt;/p&gt;

&lt;p&gt;さて，ハードマージンSVMの最適化問題は以下で与えられます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\min_{\boldsymbol{w},b} &amp;\ \|\boldsymbol{w}\|^2 \\
s.t. &amp;\ y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b) \geq 1 \ \forall i=1,\ldots,n
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;しかし，与えられたデータに対して，この制約を満たす超平面が存在しない場合，実行可能領域が空集合となり，この最適化問題が解を持たなくなってしまいます．
そこで，制約を緩和するために，全てのサンプルに対して&lt;script type=&quot;math/tex&quot;&gt;\xi_i \geq 0&lt;/script&gt;を導入し，最適化問題を以下のように書き換えます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\min_{\boldsymbol{w},b,\{\xi_i\}_{i=1}^n} &amp;\ \|\boldsymbol{w}\|^2 + C \sum_{i=1}^n \xi_i\\
s.t. &amp;\ y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b) \geq 1-\xi_i \ \forall i=1,\ldots,n\\
&amp;\ \xi_i \geq 0 \ \forall i=1,\ldots,n
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;これは，訓練データでの誤識別をある程度許容することに相当します．
我々のゴールは汎化能力の獲得，つまり，未知のデータに対する誤識別率を最小化することなので，訓練データを全て正しく識別する必要はありません．
さらに，訓練データに完全にフィットさせることは，汎化能力の低下を引き起こす原因にもなります．そのような意味でも，制約の緩和は有用です．&lt;/p&gt;

&lt;p&gt;この最適化問題では，&lt;script type=&quot;math/tex&quot;&gt;\xi_i&gt;0&lt;/script&gt;の時，&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{x}_i&lt;/script&gt;の誤識別を許容します．
さらに，&lt;script type=&quot;math/tex&quot;&gt;\xi_i&lt;/script&gt;は小さい方が望ましいので，目的関数に加えて同時に最小化します．
&lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;は識別率をコントロールするハイパーパラメータです．&lt;/p&gt;

&lt;p&gt;さて，ここで，&lt;script type=&quot;math/tex&quot;&gt;\xi_i&lt;/script&gt;を以下のように定義すると，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\xi_i = \max\{0,1-y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b)\}
\end{align*},&lt;/script&gt;

&lt;p&gt;上の最適化問題は，以下のように書き換えられます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\min_{\boldsymbol{w},b} \ \sum_{i=1}^n \max\{0,1-y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b)\} + \lambda \|\boldsymbol{w}\|^2
\end{align*}&lt;/script&gt;

&lt;p&gt;ここで，&lt;script type=&quot;math/tex&quot;&gt;\max\{0,1-y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b)\}&lt;/script&gt;を損失関数とみると，経験リスク最小化学習になっていることがわかります．損失関数&lt;script type=&quot;math/tex&quot;&gt;\ell_{hinge}(t) = \max\{0,1-t\}&lt;/script&gt;はヒンジ損失と呼ばれています．&lt;/p&gt;

&lt;p&gt;ここから実装について，書きたいと思います．
&lt;a rel=&quot;nofollow&quot; href=&quot;http://www.amazon.co.jp/gp/product/406152903X/ref=as_li_ss_tl?ie=UTF8&amp;amp;camp=247&amp;amp;creative=7399&amp;amp;creativeASIN=406152903X&amp;amp;linkCode=as2&amp;amp;tag=nettodesyuu00-22&quot; target=&quot;_blank&quot;&gt;オンライン機械学習 (機械学習プロフェッショナルシリーズ)&lt;/a&gt;&lt;img src=&quot;http://ir-jp.amazon-adsystem.com/e/ir?t=nettodesyuu00-22&amp;amp;l=as2&amp;amp;o=9&amp;amp;a=406152903X&quot; width=&quot;1&quot; height=&quot;1&quot; border=&quot;0&quot; alt=&quot;&quot; style=&quot;border:none !important; margin:0px !important;&quot; /&gt;を参考にしています．
SVMは二次計画法で解くこともできますが，今回は確率的勾配法（正確にはIncremental Gradient Method）を使って解きたいと思います．
&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{x} \in \mathbb{R}^{d+1}&lt;/script&gt; として，簡単のために &lt;script type=&quot;math/tex&quot;&gt;x_1 = 1&lt;/script&gt;とします．すると，切片&lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;が&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}&lt;/script&gt;に吸収され，上の最適化問題は，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\min_{\boldsymbol{w},b} \ \sum_{i=1}^n \max\{0,1-y_i\boldsymbol{w}^T \boldsymbol{x}_i\} + \lambda \|\boldsymbol{w}\|^2
\end{align*}&lt;/script&gt;

&lt;p&gt;と書けます．
確率的勾配法では，適当にサンプル&lt;script type=&quot;math/tex&quot;&gt;(\boldsymbol{x}_i,y_i)&lt;/script&gt;を一つ取り出して，それに対応する目的関数，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
J_i(\boldsymbol{w}) = \max\{0,1-y_i\boldsymbol{w}^T \boldsymbol{x}_i\} + \lambda \|\boldsymbol{w}\|^2
\end{align*},&lt;/script&gt;

&lt;p&gt;の勾配を求めます．その勾配&lt;script type=&quot;math/tex&quot;&gt;\nabla J_i(\boldsymbol{w})&lt;/script&gt;を用いて，パラメータを&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w} \leftarrow \boldsymbol{w} - \eta\nabla J_i(\boldsymbol{w})&lt;/script&gt;と更新します．これを収束まで繰り返します．&lt;/p&gt;

&lt;p&gt;しかし，ヒンジ損失は&lt;script type=&quot;math/tex&quot;&gt;1-y_i\boldsymbol{w}^T \boldsymbol{x}_i = 0&lt;/script&gt;を満たす点で微分不可能なため，劣勾配を考えます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\nabla J_i(\boldsymbol{w}) = \left\{
\begin{array}{ll}
-y_i\boldsymbol{x}_i + 2 \lambda \boldsymbol{w} &amp; (1-y_i\boldsymbol{w}^T \boldsymbol{x}_i \geq 0) \\
2 \lambda \boldsymbol{w} &amp; otherwise
\end{array}\right.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;これを踏まえると，アルゴリズムは以下のようになります．&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ランダムに&lt;script type=&quot;math/tex&quot;&gt;(\boldsymbol{x}_i,y_i) \in \{(\boldsymbol{x}_i,y_i)\}_{i=1}^n&lt;/script&gt;を取り出す．&lt;/li&gt;
  &lt;li&gt;以下のように&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}&lt;/script&gt;を更新 (&lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;はステップサイズ)&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\boldsymbol{w} \leftarrow
\boldsymbol{w} - \eta\left(2 \lambda \boldsymbol{w} + 
\left\{
\begin{array}{ll}
-y_i\boldsymbol{x}_i &amp; (1-y_i\boldsymbol{w}^T \boldsymbol{x}_i \geq 0) \\
0 &amp; otherwise
\end{array}\right.\right)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;実装してみました．
&lt;script src=&quot;https://gist.github.com/nktmemoja/15c7120a873f6195ee86.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;結果は以下のようになりました．
&lt;img src=&quot;/assets/softmarginsvm_demo.png&quot; alt=&quot;softmarginsvm_demo.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;なお，以下に書いたように，実装においてはステップサイズの決定が非常に難しいです．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;[http://nktmemoja.github.io/jekyll/update/2016/01/10/gradient-method.html]&quot;&gt;勾配法で目的関数値は単調に減少していくのか？&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;実際に使用する時は，scikit-learnのsklearn.linear_model.SGDClassifierを使うのが賢明でしょう．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html&quot; target=&quot;_blank&quot;&gt;http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;今回はソフトマージンSVMを定式化し，確率的勾配法を用いて実装してみた．
とりあえず簡単な人工データでうまく動くことを確認した．
この実装は実用に耐えうるものではないので，簡単なデモだと思ってください^^．
読んでいただき，ありがとうございました．&lt;/p&gt;

</description>
        <pubDate>Mon, 11 Jan 2016 12:43:59 +0900</pubDate>
        <link>http://nktmemoja.github.io/jekyll/update/2016/01/11/softmarginsvm.html</link>
        <guid isPermaLink="true">http://nktmemoja.github.io/jekyll/update/2016/01/11/softmarginsvm.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>勾配法で目的関数値は単調に減少していくのか？</title>
        <description>&lt;p&gt;目的関数&lt;script type=&quot;math/tex&quot;&gt;f: \mathbb{R}^d \rightarrow \mathbb{R}&lt;/script&gt;の最小化を考えましょう．
勾配法では，以下のようにパラメータ&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{x}&lt;/script&gt;を更新していきます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\boldsymbol{x}_{k+1} = x_k - h_k \nabla f(\boldsymbol{x}_k), \ k=0,1,\ldots
\end{align*}&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;h_k &gt; 0&lt;/script&gt;はステップサイズ．&lt;/p&gt;

&lt;p&gt;1ステップ分の更新を考えましょう．
&lt;script type=&quot;math/tex&quot;&gt;y = \boldsymbol{x} - h_k \nabla f(\boldsymbol{x})&lt;/script&gt;とすると，我々が示したいのは，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
f(\boldsymbol{y}) \leq f(\boldsymbol{x})
\end{align*}.&lt;/script&gt;

&lt;p&gt;これだけではなにも議論できないので，これからは目的関数はリプシッツ連続 (Lipschitz continuous) だとします．
ではまずリプシッツ連続の定義から始めましょう．&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Q \subseteq \mathbb{R}^d, L&gt;0&lt;/script&gt;に対して，集合&lt;script type=&quot;math/tex&quot;&gt;C^{k,p}_L(Q)&lt;/script&gt;を以下の性質を有する集合とします．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;f \in C^{k,p}_L(Q)&lt;/script&gt; はk回微分可能で連続&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;f^{(p)}&lt;/script&gt;を&lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;のp階微分とすると，
&lt;script type=&quot;math/tex&quot;&gt;\|f^{(p)}(\boldsymbol{x}) - f^{(p)}(\boldsymbol{y})\|_2 \leq L\|\boldsymbol{x}-\boldsymbol{y}\|_2 \ \forall \boldsymbol{x},\boldsymbol{y} \in Q&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;この時&lt;script type=&quot;math/tex&quot;&gt;f\in C^{k,p}_L(Q)&lt;/script&gt;はリプシッツ連続であるという．&lt;/p&gt;

&lt;p&gt;今，目的関数が&lt;script type=&quot;math/tex&quot;&gt;f \in C^{1,1}_L(\mathbb{R}^d)&lt;/script&gt;だとしましょう．この時，以下が成立します．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
|f(\boldsymbol{y})-f(\boldsymbol{x})-\langle f&#39;(\boldsymbol{x}),\boldsymbol{y}-\boldsymbol{x} \rangle| \leq \frac{L}{2}\|\boldsymbol{y}-\boldsymbol{x}\|_2^2 \ \cdots (1)
\end{align*}&lt;/script&gt;

&lt;p&gt;さて，再び勾配法による一回分の更新を考えましょう．
&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{y}=\boldsymbol{x}-h\nabla f(\boldsymbol{x})&lt;/script&gt;とすると，(1)より，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
f(\boldsymbol{y}) &amp;\leq f(\boldsymbol{x}) + \langle \nabla f(\boldsymbol{x}), \boldsymbol{y}-\boldsymbol{x} \rangle + \frac{L}{2}\|\boldsymbol{y}-\boldsymbol{x}\|^2_2 \\
&amp;= f(\boldsymbol{x}) - h\left(1 - \frac{h}{2}L\right)\|\nabla f(\boldsymbol{x})\|^2_2
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;したがって，&lt;script type=&quot;math/tex&quot;&gt;h\left(1 - \frac{h}{2}L\right) \geq 0&lt;/script&gt;，つまり，&lt;script type=&quot;math/tex&quot;&gt;h \leq \frac{2}{L}&lt;/script&gt;であれば，&lt;script type=&quot;math/tex&quot;&gt;f(\boldsymbol{y}) \leq f(\boldsymbol{x})&lt;/script&gt;が言えます．
つまり，当たり前かもしれませんが，勾配法によって関数が単調に減少するかはステップサイズの決め方に依存するということです．
ここから，ステップサイズの決め方がいかに重要かがわかりますね．&lt;/p&gt;

&lt;p&gt;問題なのは&lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt;がわからないということです．
もっともナイーブなステップサイズの決定方法は&lt;script type=&quot;math/tex&quot;&gt;h_k = h \ \forall k=0,1,\ldots&lt;/script&gt;として，適当に&lt;script type=&quot;math/tex&quot;&gt;h&gt;0&lt;/script&gt;を決めてやることですが，これだと必ずしも&lt;script type=&quot;math/tex&quot;&gt;h \leq \frac{2}{L}&lt;/script&gt;となっている保証はありません．そこで，なんらかの工夫が必要です．&lt;/p&gt;

&lt;p&gt;基本的には，更新の際に，&lt;script type=&quot;math/tex&quot;&gt;f(\boldsymbol{y}) \leq f(\boldsymbol{x})&lt;/script&gt;となる&lt;script type=&quot;math/tex&quot;&gt;h&gt;0&lt;/script&gt;を選べば良いわけですが，どうやったら良いのでしょうか？
適当に&lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt;を決めましょう．この時，&lt;script type=&quot;math/tex&quot;&gt;f(\boldsymbol{y}) &gt; f(\boldsymbol{x})&lt;/script&gt;となってしまったとします．
この時の戦略は，&lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt;を大きくするか，小さくするかです．どうしたらいいでしょう？&lt;/p&gt;

&lt;p&gt;もしも，更新によって関数値が増加してしまう時，つまり&lt;script type=&quot;math/tex&quot;&gt;f(\boldsymbol{y})&gt;f(\boldsymbol{x})&lt;/script&gt;の時，ステップサイズが&lt;script type=&quot;math/tex&quot;&gt;h&gt;\frac{2}{L}&lt;/script&gt;となっていることがわかります．ここから，&lt;script type=&quot;math/tex&quot;&gt;f(\boldsymbol{y}) \leq f(\boldsymbol{x})&lt;/script&gt;となるまで&lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt;を小さくしていけば良いことがわかります．
これは，更新によって，局所解または最適解を通り過ぎてしまったので，通り過ぎない程度に引き戻すことに相当します．
こんな感じでステップサイズを決めれば，一応目的関数は単調に減少していきます．&lt;/p&gt;

&lt;p&gt;まとめると，勾配法を使う際に最低限考えなければならないことは，&lt;script type=&quot;math/tex&quot;&gt;f(\boldsymbol{x}_{k+1}) \leq f(\boldsymbol{x}_k)&lt;/script&gt;となるように更新することで，これはステップサイズの決定に委ねられる．ステップサイズは，&lt;script type=&quot;math/tex&quot;&gt;f(\boldsymbol{x}_{k+1}) \leq f(\boldsymbol{x}_k)&lt;/script&gt;を満たすように決めれば良いが，もしこれが満たされない場合はステップサイズを小さくしていけば良い．&lt;/p&gt;

&lt;p&gt;こんな感じですかね．実際は，ステップサイズの決め方にもGoldstein-Armijo ruleとか，いろいろあるのでそれを使えばいいんですけどね．&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Jan 2016 06:20:07 +0900</pubDate>
        <link>http://nktmemoja.github.io/jekyll/update/2016/01/11/gradient-method.html</link>
        <guid isPermaLink="true">http://nktmemoja.github.io/jekyll/update/2016/01/11/gradient-method.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>ハードマージンSupport Vector Machine (SVM)の定式化</title>
        <description>&lt;p&gt;&lt;a rel=&quot;nofollow&quot; href=&quot;http://www.amazon.co.jp/gp/product/4061529064/ref=as_li_ss_tl?ie=UTF8&amp;amp;camp=247&amp;amp;creative=7399&amp;amp;creativeASIN=4061529064&amp;amp;linkCode=as2&amp;amp;tag=nettodesyuu00-22&quot; target=&quot;_blank&quot;&gt;サポートベクトルマシン (機械学習プロフェッショナルシリーズ)&lt;/a&gt;&lt;img src=&quot;http://ir-jp.amazon-adsystem.com/e/ir?t=nettodesyuu00-22&amp;amp;l=as2&amp;amp;o=9&amp;amp;a=4061529064&quot; width=&quot;1&quot; height=&quot;1&quot; border=&quot;0&quot; alt=&quot;&quot; style=&quot;border:none !important; margin:0px !important;&quot; /&gt;
を読んでいます．
SVMについて，わかっている気になっていましたが，わかっていませんでした．
意外と奥が深いですね．というわけでその整理の意味も込めてのポスト．&lt;/p&gt;

&lt;p&gt;この記事ではハードマージンSVMを考えます．
ハードマージンSVMでは，与えられたデータは線形分離可能であると仮定します．
つまり，超平面&lt;script type=&quot;math/tex&quot;&gt;f(\boldsymbol{x})=\boldsymbol{w}^T \boldsymbol{x} + b&lt;/script&gt;で，与えられた訓練データがミスなく分離できることを仮定します．
このような超平面は複数存在すると考えられますが，
SVMでは，「マージンを最大化する超平面を求める」，というのはよくある説明ですね．&lt;/p&gt;

&lt;p&gt;ではまず，マージンの定義から入りましょう．
今，訓練データ&lt;script type=&quot;math/tex&quot;&gt;\{(\boldsymbol{x}_i,y_i)\}_{i=1}^n, \boldsymbol{x}_i \in \mathbb{R}^d, y_i \in \{-1,1\} \ \forall i=1, \ldots, n&lt;/script&gt;が与えられているとします．
マージンを，
超平面&lt;script type=&quot;math/tex&quot;&gt;f(\boldsymbol{x})=\boldsymbol{w}^T \boldsymbol{x} + b&lt;/script&gt;からもっとも近いサンプルまでの距離と定義します．
つまり，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
i^{*} = \mathop{\rm arg\,min}\limits_{1 \leq i \leq n} \ \frac{|\boldsymbol{w}^T \boldsymbol{x}_i + b|}{\|\boldsymbol{w}\|}
= \mathop{\rm arg\,min}\limits_{1 \leq i \leq n} \ |\boldsymbol{w}^T \boldsymbol{x}_i + b|
\end{align*}&lt;/script&gt;

&lt;p&gt;と定義すると，マージンは&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\frac{|\boldsymbol{w}^T \boldsymbol{x}_{i^{*}} + b|}{\|\boldsymbol{w}\|}
\end{align*}&lt;/script&gt;

&lt;p&gt;と表すことができます．
全ての訓練データを正しく分類しつつ，このマージンを最大化したいので，以下の最適化問題を解きます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\max_{\boldsymbol{w},b} &amp;\ \frac{|\boldsymbol{w}^T \boldsymbol{x}_{i^{*}} + b|}{\|\boldsymbol{w}\|} \\
s.t. &amp;\ y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b) &gt; 0 \ \forall i=1,\ldots ,n
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;分子が邪魔なので，これを以下のように書き換えます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\max_{\boldsymbol{w},b} &amp;\ \frac{1}{\|\boldsymbol{w}\|} \\
s.t. &amp;\ y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b) &gt; 0 \ \forall i=1,\ldots ,n \\
&amp;\ |\boldsymbol{w}^T \boldsymbol{x}_{i^{*}} + b |=1
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;二つの制約条件を一つにまとめると，以下のようになります．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\max_{\boldsymbol{w},b} &amp;\ \frac{1}{\|\boldsymbol{w}\|} \\
s.t. &amp;\ y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b) \geq 1 \ \forall i=1,\ldots ,n \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;個人的にはここの変形が一番きつかったですね…．
なので軽く補足しときます．
以下の制約を書き換えていきます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
 y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b) &gt; 0 \ \forall i=1,\ldots ,n \ \cdots (1) \\
 |\boldsymbol{w}^T \boldsymbol{x}_{i^{*}} + b |=1 \ \cdots (2)
\end{align}&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;i^{*}&lt;/script&gt;の定義から，(2)は以下のように書き換えられます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\ |\boldsymbol{w}^T \boldsymbol{x}_{i} + b| \geq 1  \ \forall i=1,\ldots ,n \ \cdots (2&#39;)
\end{align}&lt;/script&gt;

&lt;p&gt;ここで，(1)と(2’)を合体させたいのですが，
&lt;script type=&quot;math/tex&quot;&gt;y_i(\boldsymbol{w}^T \boldsymbol{x}_{i} + b) = |\boldsymbol{w}^T \boldsymbol{x}_{i} + b|&lt;/script&gt;
を示すことができれば良さそうです．
ここで，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b) = \left\{
\begin{array}
\boldsymbol{w}^T \boldsymbol{x}_i + b &amp; (y_i=1) \\
-(\boldsymbol{w}^T \boldsymbol{x}_i + b) &amp; (y_i=-1)
\end{array}
\right.
\end{align*}, %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
|\boldsymbol{w}^T \boldsymbol{x}_i + b| = \left\{
\begin{array}
\boldsymbol{w}^T \boldsymbol{x}_i + b &amp; (\boldsymbol{w}^T \boldsymbol{x}_i + b \geq 0) \\
-(\boldsymbol{w}^T \boldsymbol{x}_i + b) &amp; (\boldsymbol{w}^T \boldsymbol{x}_i + b &lt; 0)
\end{array}
\right.
\end{align*}, %]]&gt;&lt;/script&gt;

&lt;p&gt;ですが，(1)より，&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}^T \boldsymbol{x}_i + b \geq 0&lt;/script&gt; は成り立たず，この文脈では必ず
&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}^T \boldsymbol{x}_i + b &gt; 0&lt;/script&gt;となります．&lt;/p&gt;

&lt;p&gt;なので，&lt;script type=&quot;math/tex&quot;&gt;y_i=1 \Leftrightarrow \boldsymbol{w}^T \boldsymbol{x}_i + b &gt; 0&lt;/script&gt;と
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
y_i=-1 \Leftrightarrow \boldsymbol{w}^T \boldsymbol{x}_i + b &lt; 0 %]]&gt;&lt;/script&gt;を示すことにします．
すごく簡単すぎて示す必要もないかもしれませんが…．&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;y_i=1&lt;/script&gt;の時，(1)より，&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}^T \boldsymbol{x}_i + b &gt; 0&lt;/script&gt;．
&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}^T \boldsymbol{x}_i + b &gt; 0&lt;/script&gt;の時，(1)と&lt;script type=&quot;math/tex&quot;&gt;y_i \in \{-1,1\}&lt;/script&gt;より，&lt;script type=&quot;math/tex&quot;&gt;y_i=1&lt;/script&gt;．
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
y_i=-1 \Leftrightarrow \boldsymbol{w}^T \boldsymbol{x}_i + b &lt; 0 %]]&gt;&lt;/script&gt;も同様．&lt;/p&gt;

&lt;p&gt;よって，
&lt;script type=&quot;math/tex&quot;&gt;y_i(\boldsymbol{w}^T \boldsymbol{x}_{i} + b) = |\boldsymbol{w}^T \boldsymbol{x}_{i} + b|&lt;/script&gt;．&lt;/p&gt;

&lt;p&gt;そういうわけで，(2’)は，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\ y_i(\boldsymbol{w}^T \boldsymbol{x}_{i} + b) \geq 1  \ \forall i=1,\ldots ,n \ \cdots (2&#39;&#39;)
\end{align}&lt;/script&gt;

&lt;p&gt;これで，(1)と(2’‘)を合体できますね．
最後に最大化と最小化を書き換えて，ノルムを2乗すれば，よく見るハードマージンSVMの最適化問題の完成です：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\min_{\boldsymbol{w},b} &amp;\ \|\boldsymbol{w}\|^2 \\
s.t. &amp;\ y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b) \geq 1 \ \forall i=1,\ldots ,n 
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;ここで，与えられたデータに対して，制約&lt;script type=&quot;math/tex&quot;&gt;y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b) \geq 1 \ \forall i=1,\ldots ,n&lt;/script&gt;を満たす&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}&lt;/script&gt;と&lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;が存在しないかもしれません．
というより，現実問題ではそのような場合がほとんどだと考えられます．
そこで，ソフトマージンの考え方が出てきます．
これについてはまた後日書けたらと思います．&lt;/p&gt;
</description>
        <pubDate>Fri, 08 Jan 2016 00:30:29 +0900</pubDate>
        <link>http://nktmemoja.github.io/jekyll/update/2016/01/08/hardmarginsvmformulation.html</link>
        <guid isPermaLink="true">http://nktmemoja.github.io/jekyll/update/2016/01/08/hardmarginsvmformulation.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>正例とラベル無しデータからの学習 (PU classification)</title>
        <description>&lt;p&gt;通常の2値分類問題では，正例と負例が与えられています． しかし扱う問題によっては，このようなデータを用意するのが困難な時があります． 例えば，抽出型のタスクです． 抽出型のタスクでは，抽出したい対象を正例と考えます． この場合の負例は「正例以外のデータ」と定義するほかありません． しかし，集めた正例に対し，それ以外のデータを負例と定義してしまうと， それ以外のデータに含まれる正例も負例として扱ってしまいます．&lt;/p&gt;

&lt;p&gt;このように負例を定義するのが難しい場合には，正例とラベルなしデータから学習する枠組み，PU classificationが有用です． PU classificationについては，Elkan and Noto 2008を参照していただければと思うのですが，少しだけ解説しておきます． 3つの確率変数&lt;script type=&quot;math/tex&quot;&gt;x,y,s&lt;/script&gt;を考えます．ここで，&lt;script type=&quot;math/tex&quot;&gt;x \in \mathbb{R}, y \in \{-1,1\}, s \in \{0,1\}&lt;/script&gt;だとします． &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;は入力，&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;はクラスラベル，そして&lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;はデータがラベリングされるかどうかを表しています． 我々が欲しいのは，
&lt;script type=&quot;math/tex&quot;&gt;p(y|x)&lt;/script&gt;
ですが，PU classificationでは&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;は観測することができません． 我々のゴールは，&lt;script type=&quot;math/tex&quot;&gt;\{(x_i,s_i)\}_{i=1}^n&lt;/script&gt;から
&lt;script type=&quot;math/tex&quot;&gt;p(y|x)&lt;/script&gt;
を学習することです． 結果からいうと，2つの仮定をおくことで，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y=1|x) = \frac{p(s=1|x)}{p(s=1|y=1)}&lt;/script&gt;

&lt;p&gt;と表せます．
&lt;script type=&quot;math/tex&quot;&gt;p(s=1|x)&lt;/script&gt;
は与えられたデータから推定できます． そして，
&lt;script type=&quot;math/tex&quot;&gt;p(s=1|y=1)&lt;/script&gt;
は開発データから推定できます． 詳しくはElkan and Noto 2008の2章にまとめられています． 今回はElkan and Noto 2008の手法を用いてPU classificationを行っていきます．&lt;/p&gt;

&lt;p&gt;では，以下のような正解データを考えましょう． &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-true_labeled.png&quot; alt=&quot;true_labeled.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;このデータに対して，実際に与えられるのは以下のようなデータです． &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-pu_data.png&quot; alt=&quot;pu_data.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;このデータに対して，まずは通常のロジスティック回帰を適用してみます． なお，今回は負例が多いので，交差確認法には正例側のF値を用います． 結果は以下のようになりました． &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-result_of_tradclf1.png&quot; alt=&quot;result_of_tradclf.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ご覧のように，全て負例だと予測してしまいました． 次に，PU classificationを適用してみます． &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-result_of_puclassification.png&quot; alt=&quot;result_of_puclassification.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;正例とラベルなしデータから，うまく分類境界を学習できていることがわかります．&lt;/p&gt;

&lt;p&gt;デモ用のコードは以下に載せておきますのでぜひ試してみてください． ちなみに，非線形な分類境界を表現するための&lt;a href=&quot;https://gist.github.com/nkt1546789/e41199340f7a42c515be&quot; title=&quot;rbfmodel_wrapper.py&quot;&gt;rbfmodel_wrapper.py&lt;/a&gt; と PU Classificationのための&lt;a href=&quot;https://gist.github.com/nkt1546789/9fbbf2f450779bde60c3&quot; title=&quot;puwrapper.py&quot;&gt;puwrapper.py&lt;/a&gt; も合わせてDLしてください．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://gist.github.com/nkt1546789/e9421f06ea3a62bfbb8c&quot; title=&quot;https://gist.github.com/nkt1546789/e9421f06ea3a62bfbb8c&quot;&gt;https://gist.github.com/nkt1546789/e9421f06ea3a62bfbb8c&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Thu, 29 Oct 2015 21:47:00 +0900</pubDate>
        <link>http://nktmemoja.github.io/machine%20learning/2015/10/29/29214700.html</link>
        <guid isPermaLink="true">http://nktmemoja.github.io/machine%20learning/2015/10/29/29214700.html</guid>
        
        <category>Machine Learning</category>
        
        <category>PU classification</category>
        
        
        <category>Machine Learning</category>
        
      </item>
    
      <item>
        <title>Word2Vecを使った単語間関係分類</title>
        <description>&lt;p&gt;単語間には様々な関係があります． 今回は，単語間の関係をWord2Vecで学習させようと思います． Word2Vecにはアナロジーを捉えられるという話があります． あの有名な，king - man + woman = queen というやつですね． これは，king - man = queen - womanとも書けます． つまり，2語間の差が関係を表しており， この例だと，kingとmanの関係とqueenとwomanの関係が同じであると捉えることができます．&lt;/p&gt;

&lt;p&gt;さて，Word2Vecで学習したベクトル表現を使うと，差ベクトルがうまいこと関係を表すと書きましたが， 必ずしもそうなっているとは限りません． 加えて，ユーザが扱いたい関係とWord2Vecで学習した関係が一致しているとも限りません． そこで，今回も例のごとく教師あり学習を使います． ユーザは教師データを通して，扱いたい関係をアルゴリズムに伝えることができます．&lt;/p&gt;

&lt;p&gt;最初からあまり多くの関係を対象にするのはしんどいので，今回はis-a, has-a関係のみに着目します． これは，僕の理解では，柔道 is-a スポーツ，スポーツ has-a 柔道みたいなものだと思っています． まずは&lt;a href=&quot;https://gist.github.com/nkt1546789/a3b3a4c166fc1c0486a1&quot; title=&quot;training data&quot;&gt;training data&lt;/a&gt;を用意します． is-aとhas-aは反対の関係になっていると思うので，has-aのデータだけ用意します． この中には (スポーツ，野球)というhas-a関係を表す順序対がリストで格納されています． リスト内の順序対に対して差ベクトルを計算し，一つのデータとして扱います．&lt;/p&gt;

&lt;p&gt;コードは以下のようになりました．&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;training&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;gensim.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Word2Vec&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisticRegressionCV&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Word2Vec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;/path/to/your/w2v_model&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;permutation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ntr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;itr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ntr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ite&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ntr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Xtr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;itr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ytr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;itr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Xte&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;yte&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LogisticRegressionCV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xtr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ytr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ypred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xte&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yte&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ypred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# &amp;amp;gt;&amp;amp;gt;&amp;amp;gt; 0.99502487562189057&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;今回は厳密な実験はせずに，簡単に性能を見てみました． テストデータに対して，99%の精度を出すことができました． 以下簡単なテストデータへの予測例です．&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ypred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;has-a&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;is-a&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# results:&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;ブルーベリー&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;果物&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;動物&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;モルモット&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;動物&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;ワタボウシタマリン&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;スポーツ&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;スポーツ&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;登山&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;スポーツ&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;ロデオ&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;スポーツ&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;動物&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;ユーラシアカワウソ&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;スポーツ&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;フリーダイビング&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;競馬&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;スポーツ&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;スポーツ&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;ゴルフ&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;いかがでしょうか？うまく狙った関係が分類できていると思います． とりあえずはうまくいきました！これで成功です！ これがどこまで実用に耐えられるかはやってみないとわかりませんが．&lt;/p&gt;

</description>
        <pubDate>Tue, 27 Oct 2015 20:57:00 +0900</pubDate>
        <link>http://nktmemoja.github.io/machine%20learning/2015/10/27/27205700.html</link>
        <guid isPermaLink="true">http://nktmemoja.github.io/machine%20learning/2015/10/27/27205700.html</guid>
        
        <category>Machine Learning</category>
        
        <category>NLP</category>
        
        <category>python</category>
        
        
        <category>Machine Learning</category>
        
      </item>
    
  </channel>
</rss>
