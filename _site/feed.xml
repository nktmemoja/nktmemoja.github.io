<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>nktmemo_ja</title>
    <description>nktmemo provides interesting articles about Machine Learning and NLP.
</description>
    <link>http://nktmemoja.github.io/</link>
    <atom:link href="http://nktmemoja.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 07 Jul 2016 01:28:02 +0900</pubDate>
    <lastBuildDate>Thu, 07 Jul 2016 01:28:02 +0900</lastBuildDate>
    <generator>Jekyll v3.0.0</generator>
    
      <item>
        <title>ラプラス正則化を使った，半教師付きグラフノード重み付け</title>
        <description>&lt;p&gt;半教師付きの設定で，グラフノードに重み付けをつけるアルゴリズム (Laplacian Node Weighting)を考えてみました（名前適当だし，日本語でどう表すかが非常に難しい）．
グラフのノードに重みをつける手法としては，PageRankが有名です．
一つやりたいことがあって，最初PageRankを使っていたのですが，教師的な情報を組み込むのが難しかったので，自分で考えることにしました．
というのも，PageRankでは，すべてのグラフノードの重みを1に初期化します．
単純にここに教師的な情報を組み込む，例えば数個のノードの重みを高くするなど，をやってみたのですが，最終的に出てくる解にあまり反映されなかったのです．
なぜなのか，をいろいろ考えましたが，PageRankはノードの重みを繰り返し伝搬するので，初期値が大事というより，エッジが大事なんだと今は思っています（もう少しちゃんと考える必要あり）．&lt;/p&gt;

&lt;p&gt;定式化していきましょう．
グラフ&lt;script type=&quot;math/tex&quot;&gt;G=(V, E), V=\{1, \ldots, n\}, E \subseteq V \times V&lt;/script&gt;を考えます．
扱いやすいように，エッジは隣接行列 (Adjacency matrix) &lt;script type=&quot;math/tex&quot;&gt;\mathbf{A}&lt;/script&gt;で表現することにします．
&lt;script type=&quot;math/tex&quot;&gt;\mathbf{A}&lt;/script&gt;をいろいろ変えることで，いろんなグラフが表現できますが，ここでは一般の行列にしときます．
重み付けにおける教師データとは何か？という話になると思うのですが，ここでは，高い（低い）重みがついて欲しいノードの集合&lt;script type=&quot;math/tex&quot;&gt;V^{*} \subseteq V&lt;/script&gt;が与えられることにします．&lt;/p&gt;

&lt;p&gt;さて，各ノードにおける重みを&lt;script type=&quot;math/tex&quot;&gt;\mathbf{f} = [f_1 \cdots f_n]&lt;/script&gt;と表すことにして，これを求めることがゴールになります．
とりあえず，&lt;script type=&quot;math/tex&quot;&gt;V^{*}&lt;/script&gt;の要素に該当する重みを大きくしたいので，&lt;script type=&quot;math/tex&quot;&gt;\mathbf{u} = [u_1 \cdots u_n]&lt;/script&gt;を以下で定義します．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
u_i = \left\{\begin{array}{cc}
1 &amp; if \ i \in V^{*} \\
0 &amp; otherwise
\end{array}\right.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;こうすると，&lt;script type=&quot;math/tex&quot;&gt;V^{*}&lt;/script&gt;のスコアの合計値を，&lt;script type=&quot;math/tex&quot;&gt;\mathbf{f}^T \mathbf{u}&lt;/script&gt;と表すことができます．
今，&lt;script type=&quot;math/tex&quot;&gt;\mathbf{f}^T \mathbf{u}&lt;/script&gt;を最大化すれば良さそうですが，今回はラプラス正則化 (Laplacian Regularization) と呼ばれるものを加えます．
これは，ノード間にエッジが存在する場合，それらのスコアを近づける，というアイデアです．
ノード&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;と&lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;の間にエッジが存在する，すなわち&lt;script type=&quot;math/tex&quot;&gt;A_{ij}=1&lt;/script&gt;なら&lt;script type=&quot;math/tex&quot;&gt;(f_i - f_j)^2&lt;/script&gt;を最小化します．
これは，&lt;script type=&quot;math/tex&quot;&gt;A_{ij}(f_i - f_j)^2&lt;/script&gt;と書けますね．これをすべてのノードに対してやります．
以上を踏まえて，以下の問題を解きます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
max_{\mathbf{f}} \ \mathbf{f}^T \mathbf{u} - \alpha \|\mathbf{f}\|^2 - \beta \sum_{i,j=1}^n A_{ij}(f_i - f_j)^2
\end{align*}&lt;/script&gt;

&lt;p&gt;ただし，第2項にl2正則を加えています．
第3項がラプラス正則化です．
これの解は以下のように解析的に得られます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathbf{f} = (\mathbf{L} + \gamma \mathbf{I}_n)^{-1} \mathbf{u}
\end{align*}&lt;/script&gt;

&lt;p&gt;ただし，&lt;script type=&quot;math/tex&quot;&gt;\gamma=\frac{\alpha}{\beta}&lt;/script&gt;であり，これは通常の正則化パラメータと同じように事前に決めてやるものとします．
これで，望ましい結果というのを，&lt;script type=&quot;math/tex&quot;&gt;\mathbf{A}&lt;/script&gt;だけでなく，&lt;script type=&quot;math/tex&quot;&gt;\mathbf{u}&lt;/script&gt;でも表現できるようになりました．
次回は，これをウェブページ解析に応用したいと思います．&lt;/p&gt;
</description>
        <pubDate>Thu, 07 Jul 2016 00:48:19 +0900</pubDate>
        <link>http://nktmemoja.github.io/jekyll/update/2016/07/07/laplaciannodeweighting.html</link>
        <guid isPermaLink="true">http://nktmemoja.github.io/jekyll/update/2016/07/07/laplaciannodeweighting.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>Topic Modelの最尤推定の解の導出</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://www.amazon.co.jp/gp/product/4061529048/ref=as_li_ss_tl?ie=UTF8&amp;amp;camp=247&amp;amp;creative=7399&amp;amp;creativeASIN=4061529048&amp;amp;linkCode=as2&amp;amp;tag=nettodesyuu00-22&quot;&gt;トピックモデル (機械学習プロフェッショナルシリーズ)&lt;/a&gt;&lt;img src=&quot;http://ir-jp.amazon-adsystem.com/e/ir?t=nettodesyuu00-22&amp;amp;l=as2&amp;amp;o=9&amp;amp;a=4061529048&quot; width=&quot;1&quot; height=&quot;1&quot; border=&quot;0&quot; alt=&quot;&quot; style=&quot;border:none !important; margin:0px !important;&quot; /&gt;の3.4章の解の導出です．LaTeX書くのが面倒なので画像で．．．笑&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://nktmemoja.github.io/assets/CDEA4F6D-2A70-4053-87F7-B5D307125B2C.jpg&quot; alt=&quot;topic modelの最尤推定の解の導出1&quot; /&gt;
&lt;img src=&quot;http://nktmemoja.github.io/assets/BE1823C6-24EB-440A-A6CC-61E15CC7C645.jpg&quot; alt=&quot;topic modelの最尤推定の解の導出1&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 01 Jul 2016 01:07:03 +0900</pubDate>
        <link>http://nktmemoja.github.io/jekyll/update/2016/07/01/topicmodel.html</link>
        <guid isPermaLink="true">http://nktmemoja.github.io/jekyll/update/2016/07/01/topicmodel.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>Adjusted Rand Index (ARI) について勉強してみた</title>
        <description>&lt;p&gt;クラスタリングの性能評価でよくAdjusted Rand Index (ARI) が使われます．
僕もよく使っていますが，お恥ずかしながらよく知らずに使っていました．
というのも，Rand Index (RI) は理解できるのですが，ARIのAdjustedの意味がよくわかりませんでした．
というわけで，勉強してみました．&lt;/p&gt;

&lt;p&gt;まず，クラスタリングは，サンプル&lt;script type=&quot;math/tex&quot;&gt;S = \{o_i\}_{i=1}^n&lt;/script&gt;を&lt;script type=&quot;math/tex&quot;&gt;k (1 \leq k \leq n)&lt;/script&gt;個の互いに素なグループに分けることを目的とします．
この記事では，真のクラスタリング結果と，クラスタリングアルゴリズムの結果が似ているかどうかを測ることを目的とします．
なお，ここでは，クラスタリングの結果をクラスタラベルで表現することにします．&lt;/p&gt;

&lt;p&gt;さて，&lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt;に対する，
真のクラスタラベルを&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{\ell}=[\ell_1 \cdots \ell_n], \ell_i \in \{1, \ldots, k\} \forall i=1, \ldots, n&lt;/script&gt;，
クラスタリングアルゴリズムによるクラスタラベルを&lt;script type=&quot;math/tex&quot;&gt;\hat{ \boldsymbol{\ell}}=[\hat{\ell}_1 \cdots \hat{\ell}_n], \hat{\ell}_i \in \{1, \ldots, \hat{k}\} \forall i=1, \ldots, n&lt;/script&gt;とします．
クラスタリングの結果は，分類のようにラベルで考えるのではなく，ラベルのペアで考えます．
つまり，「このサンプルとこのサンプルは同じクラスタに属するか否か」で性能を評価します．
というわけで，&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{\ell}&lt;/script&gt;と &lt;script type=&quot;math/tex&quot;&gt;\hat{\boldsymbol{\ell}}&lt;/script&gt;を以下のように表現し直します．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathcal{X} &amp;= \{I(\ell_i = \ell_j) | (i,j) \in C(n, 2)\} = \{x_i\}_{i=1}^m\\
\mathcal{Y} &amp;= \{I(\hat{\ell}_i = \hat{\ell}_j) | (i,j) \in C(n, 2)\} = \{y_i\}_{i=1}^m
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;ただし，&lt;script type=&quot;math/tex&quot;&gt;m={}_n C _2&lt;/script&gt;．
さて，ここで，Rand Indexの定義を見てみます．
正規化されていないRand Indexは次式で定義されます（自己流です）．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
RI(x, y) = \sum_{i=1}^m I(x_i=y_i)
\end{align*}&lt;/script&gt;

&lt;p&gt;ただし，&lt;script type=&quot;math/tex&quot;&gt;I&lt;/script&gt;は指示関数で，&lt;script type=&quot;math/tex&quot;&gt;I(true) = 1, I(false) = 0&lt;/script&gt;です．
この式は，&lt;script type=&quot;math/tex&quot;&gt;x_i=y_i&lt;/script&gt; の数を表しており，二つのクラスタリングがどのくらい似ているかを表しています．&lt;/p&gt;

&lt;p&gt;よくRIの欠点として，「二つのクラスタリングに相関がなくても，高い値をとってしまう」という説明がなされます．
これの意味は，「適当（ランダム）にクラスタリングしても高い値をとってしまう」ということだと思います．
&lt;script type=&quot;math/tex&quot;&gt;x_i = 0&lt;/script&gt;となる&lt;script type=&quot;math/tex&quot;&gt;\mathcal{X}&lt;/script&gt;内の要素数が，&lt;script type=&quot;math/tex&quot;&gt;x_i = 1&lt;/script&gt;となる要素数に比べて圧倒的に多いので，
例えば「全てを異なるクラスタに分ける」というクラスタリングを行うと，&lt;script type=&quot;math/tex&quot;&gt;y_i = 0&lt;/script&gt;となり，RIの値が高くなってしまいます．
これは，分類問題において，クラスバランスが偏った状態で精度を使うのが適切ではない理由に似ています．&lt;/p&gt;

&lt;p&gt;このような問題を解決しようと提案されたのがARIです．
ARIでは，適当に行われたであろうクラスタリングにペナルティを与えます．
どのようなペナルティが適切でしょうか？
ARIでは，ペナルティ＝「適当にクラスタリングした時のRIの値」と考えます．
以下このペナルティを求めていきます．&lt;/p&gt;

&lt;p&gt;まず，二つのデータ&lt;script type=&quot;math/tex&quot;&gt;\mathcal{X}, \mathcal{Y}&lt;/script&gt;がぞれぞれ独立に同一の分布&lt;script type=&quot;math/tex&quot;&gt;p(x), p(y)&lt;/script&gt;から生成されたと仮定します：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathcal{X} \stackrel{i.i.d.}{\sim} p(x)\\
\mathcal{Y} \stackrel{i.i.d.}{\sim} p(y)
\end{align*}&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt;と&lt;script type=&quot;math/tex&quot;&gt;p(y)&lt;/script&gt;はそれぞれ確率変数&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;と&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;の従う分布に対応する確率密度関数を表します．
まず，上で定義したペナルティ中の「適当にクラスタリングした時」というのを，「&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;と&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;が独立な時」と定義します．
そうすると，求めるペナルティは「&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;と&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;が独立な時のRIの値」となり，具体的になりました．
そのようなペナルティは，以下で求められます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathbb{E}_{(x,y) \sim p(x,y)}\left[RI\right]
&amp;= \mathbb{E}_{(x,y) \sim p(x,y)}\left[ \sum_{i=1}^m I(x_i=y_i) \right] \\
&amp;= \sum_{x=0}^1 \sum_{y=0}^1 \sum_{i=1}^m I(x_i=y_i) p(x, y) \\
&amp;= \sum_{i=1}^m \left( p(x=1, y=1) + p(x=0, y=0) \right) \\
&amp;= m \left( p(x=1, y=1) + p(x=0, y=0) \right) \\
&amp;= m \left( p(x=1)p(y=1) + p(x=0)p(y=0) \right)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;最後の変形は&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;と&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;の独立性を使いました．
&lt;script type=&quot;math/tex&quot;&gt;p(x=0), p(x=1), p(y=0), p(y=1)&lt;/script&gt;は未知なので，データから推定してやらねばなりません．
ここでは，以下のように，最尤推定で求めることにしましょう．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(x=1) &amp;= \frac{1}{m} \sum_{i=1}^m x_i, \ p(x=0) = 1 - p(x=1) \\
p(y=1) &amp;= \frac{1}{m} \sum_{i=1}^m y_i, \ p(y=0) = 1 - p(y=1)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;無事上で定義したペナルティをもとめることができました．
RIから求めたペナルティを引けば，ARIの完成になります：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
ARI(x, y) = \sum_{i=1}^m I(x_i=y_i) - \mathbb{E}_{(x,y) \sim p(x,y)}\left[RI\right]
\end{align*}&lt;/script&gt;

&lt;p&gt;通常，RIは以下のように，0から1の値をとるように正規化して使われます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
normalized RI(x, y) = \frac{1}{m} \sum_{i=1}^m I(x_i=y_i)
\end{align*}&lt;/script&gt;

&lt;p&gt;なので，正規化したARIは以下のようになります．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
normalized ARI(x, y) = \frac{\sum_{i=1}^m I(x_i=y_i) - \mathbb{E}_{(x,y) \sim p(x,y)}\left[RI\right]}{m-\mathbb{E}_{(x,y) \sim p(x,y)}\left[RI\right]} 
\end{align*}&lt;/script&gt;

&lt;p&gt;Wikipediaの定義式なんかと一致するのを示したほうが良かったですね…．
時間があれば追記しておきます．
なお，具体例は以下のページが参考になります．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://y-uti.hatenablog.jp/entry/2014/01/19/133936&quot; target=&quot;_blank&quot;&gt;http://y-uti.hatenablog.jp/entry/2014/01/19/133936&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 24 May 2016 19:46:44 +0900</pubDate>
        <link>http://nktmemoja.github.io/jekyll/update/2016/05/24/ari.html</link>
        <guid isPermaLink="true">http://nktmemoja.github.io/jekyll/update/2016/05/24/ari.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>ソフトマージンSVMのヒンジ損失最小化学習としての解釈とその実装</title>
        <description>&lt;p&gt;前回，&lt;a href=&quot;http://nktmemoja.github.io/jekyll/update/2016/01/07/hardmarginsvmformulation.html&quot;&gt;ハードマージンSVMの定式化&lt;/a&gt;について書いたので，
今回はソフトマージンSVMの定式化をしようと思います．
さらに，それをヒンジ損失最小化学習として解釈できることを示し，
最後に確率的勾配法を使って実装したいと思います．&lt;/p&gt;

&lt;p&gt;さて，ハードマージンSVMの最適化問題は以下で与えられます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\min_{\boldsymbol{w},b} &amp;\ \|\boldsymbol{w}\|^2 \\
s.t. &amp;\ y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b) \geq 1 \ \forall i=1,\ldots,n
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;しかし，与えられたデータに対して，この制約を満たす超平面が存在しない場合，実行可能領域が空集合となり，この最適化問題が解を持たなくなってしまいます．
そこで，制約を緩和するために，全てのサンプルに対して&lt;script type=&quot;math/tex&quot;&gt;\xi_i \geq 0&lt;/script&gt;を導入し，最適化問題を以下のように書き換えます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\min_{\boldsymbol{w},b,\{\xi_i\}_{i=1}^n} &amp;\ \|\boldsymbol{w}\|^2 + C \sum_{i=1}^n \xi_i\\
s.t. &amp;\ y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b) \geq 1-\xi_i \ \forall i=1,\ldots,n\\
&amp;\ \xi_i \geq 0 \ \forall i=1,\ldots,n
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;これは，訓練データでの誤識別をある程度許容することに相当します．
我々のゴールは汎化能力の獲得，つまり，未知のデータに対する誤識別率を最小化することなので，訓練データを全て正しく識別する必要はありません．
さらに，訓練データに完全にフィットさせることは，汎化能力の低下を引き起こす原因にもなります．そのような意味でも，制約の緩和は有用です．&lt;/p&gt;

&lt;p&gt;この最適化問題では，&lt;script type=&quot;math/tex&quot;&gt;\xi_i&gt;0&lt;/script&gt;の時，&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{x}_i&lt;/script&gt;の誤識別を許容します．
さらに，&lt;script type=&quot;math/tex&quot;&gt;\xi_i&lt;/script&gt;は小さい方が望ましいので，目的関数に加えて同時に最小化します．
&lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;は識別率をコントロールするハイパーパラメータです．&lt;/p&gt;

&lt;p&gt;さて，ここで，&lt;script type=&quot;math/tex&quot;&gt;\xi_i&lt;/script&gt;を以下のように定義すると，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\xi_i = \max\{0,1-y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b)\}
\end{align*},&lt;/script&gt;

&lt;p&gt;上の最適化問題は，以下のように書き換えられます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\min_{\boldsymbol{w},b} \ \sum_{i=1}^n \max\{0,1-y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b)\} + \lambda \|\boldsymbol{w}\|^2
\end{align*}&lt;/script&gt;

&lt;p&gt;ここで，&lt;script type=&quot;math/tex&quot;&gt;\max\{0,1-y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b)\}&lt;/script&gt;を損失関数とみると，経験リスク最小化学習になっていることがわかります．損失関数&lt;script type=&quot;math/tex&quot;&gt;\ell_{hinge}(t) = \max\{0,1-t\}&lt;/script&gt;はヒンジ損失と呼ばれています．&lt;/p&gt;

&lt;p&gt;ここから実装について，書きたいと思います．
&lt;a rel=&quot;nofollow&quot; href=&quot;http://www.amazon.co.jp/gp/product/406152903X/ref=as_li_ss_tl?ie=UTF8&amp;amp;camp=247&amp;amp;creative=7399&amp;amp;creativeASIN=406152903X&amp;amp;linkCode=as2&amp;amp;tag=nettodesyuu00-22&quot; target=&quot;_blank&quot;&gt;オンライン機械学習 (機械学習プロフェッショナルシリーズ)&lt;/a&gt;&lt;img src=&quot;http://ir-jp.amazon-adsystem.com/e/ir?t=nettodesyuu00-22&amp;amp;l=as2&amp;amp;o=9&amp;amp;a=406152903X&quot; width=&quot;1&quot; height=&quot;1&quot; border=&quot;0&quot; alt=&quot;&quot; style=&quot;border:none !important; margin:0px !important;&quot; /&gt;を参考にしています．
SVMは二次計画法で解くこともできますが，今回は確率的勾配法（正確にはIncremental Gradient Method）を使って解きたいと思います．
&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{x} \in \mathbb{R}^{d+1}&lt;/script&gt; として，簡単のために &lt;script type=&quot;math/tex&quot;&gt;x_1 = 1&lt;/script&gt;とします．すると，切片&lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;が&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}&lt;/script&gt;に吸収され，上の最適化問題は，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\min_{\boldsymbol{w},b} \ \sum_{i=1}^n \max\{0,1-y_i\boldsymbol{w}^T \boldsymbol{x}_i\} + \lambda \|\boldsymbol{w}\|^2
\end{align*}&lt;/script&gt;

&lt;p&gt;と書けます．
確率的勾配法では，適当にサンプル&lt;script type=&quot;math/tex&quot;&gt;(\boldsymbol{x}_i,y_i)&lt;/script&gt;を一つ取り出して，それに対応する目的関数，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
J_i(\boldsymbol{w}) = \max\{0,1-y_i\boldsymbol{w}^T \boldsymbol{x}_i\} + \lambda \|\boldsymbol{w}\|^2
\end{align*},&lt;/script&gt;

&lt;p&gt;の勾配を求めます．その勾配&lt;script type=&quot;math/tex&quot;&gt;\nabla J_i(\boldsymbol{w})&lt;/script&gt;を用いて，パラメータを&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w} \leftarrow \boldsymbol{w} - \eta\nabla J_i(\boldsymbol{w})&lt;/script&gt;と更新します．これを収束まで繰り返します．&lt;/p&gt;

&lt;p&gt;しかし，ヒンジ損失は&lt;script type=&quot;math/tex&quot;&gt;1-y_i\boldsymbol{w}^T \boldsymbol{x}_i = 0&lt;/script&gt;を満たす点で微分不可能なため，劣勾配を考えます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\nabla J_i(\boldsymbol{w}) = \left\{
\begin{array}{ll}
-y_i\boldsymbol{x}_i + 2 \lambda \boldsymbol{w} &amp; (1-y_i\boldsymbol{w}^T \boldsymbol{x}_i \geq 0) \\
2 \lambda \boldsymbol{w} &amp; otherwise
\end{array}\right.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;これを踏まえると，アルゴリズムは以下のようになります．&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ランダムに&lt;script type=&quot;math/tex&quot;&gt;(\boldsymbol{x}_i,y_i) \in \{(\boldsymbol{x}_i,y_i)\}_{i=1}^n&lt;/script&gt;を取り出す．&lt;/li&gt;
  &lt;li&gt;以下のように&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}&lt;/script&gt;を更新 (&lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;はステップサイズ)&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\boldsymbol{w} \leftarrow
\boldsymbol{w} - \eta\left(2 \lambda \boldsymbol{w} + 
\left\{
\begin{array}{ll}
-y_i\boldsymbol{x}_i &amp; (1-y_i\boldsymbol{w}^T \boldsymbol{x}_i \geq 0) \\
0 &amp; otherwise
\end{array}\right.\right)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;実装してみました．
&lt;script src=&quot;https://gist.github.com/nktmemoja/15c7120a873f6195ee86.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;結果は以下のようになりました．
&lt;img src=&quot;/assets/softmarginsvm_demo.png&quot; alt=&quot;softmarginsvm_demo.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;なお，以下に書いたように，実装においてはステップサイズの決定が非常に難しいです．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;[http://nktmemoja.github.io/jekyll/update/2016/01/10/gradient-method.html]&quot;&gt;勾配法で目的関数値は単調に減少していくのか？&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;実際に使用する時は，scikit-learnのsklearn.linear_model.SGDClassifierを使うのが賢明でしょう．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html&quot; target=&quot;_blank&quot;&gt;http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;今回はソフトマージンSVMを定式化し，確率的勾配法を用いて実装してみた．
とりあえず簡単な人工データでうまく動くことを確認した．
この実装は実用に耐えうるものではないので，簡単なデモだと思ってください^^．
読んでいただき，ありがとうございました．&lt;/p&gt;

</description>
        <pubDate>Mon, 11 Jan 2016 12:43:59 +0900</pubDate>
        <link>http://nktmemoja.github.io/jekyll/update/2016/01/11/softmarginsvm.html</link>
        <guid isPermaLink="true">http://nktmemoja.github.io/jekyll/update/2016/01/11/softmarginsvm.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>勾配法で目的関数値は単調に減少していくのか？</title>
        <description>&lt;p&gt;目的関数&lt;script type=&quot;math/tex&quot;&gt;f: \mathbb{R}^d \rightarrow \mathbb{R}&lt;/script&gt;の最小化を考えましょう．
勾配法では，以下のようにパラメータ&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{x}&lt;/script&gt;を更新していきます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\boldsymbol{x}_{k+1} = x_k - h_k \nabla f(\boldsymbol{x}_k), \ k=0,1,\ldots
\end{align*}&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;h_k &gt; 0&lt;/script&gt;はステップサイズ．&lt;/p&gt;

&lt;p&gt;1ステップ分の更新を考えましょう．
&lt;script type=&quot;math/tex&quot;&gt;y = \boldsymbol{x} - h_k \nabla f(\boldsymbol{x})&lt;/script&gt;とすると，我々が示したいのは，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
f(\boldsymbol{y}) \leq f(\boldsymbol{x})
\end{align*}.&lt;/script&gt;

&lt;p&gt;これだけではなにも議論できないので，これからは目的関数はリプシッツ連続 (Lipschitz continuous) だとします．
ではまずリプシッツ連続の定義から始めましょう．&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Q \subseteq \mathbb{R}^d, L&gt;0&lt;/script&gt;に対して，集合&lt;script type=&quot;math/tex&quot;&gt;C^{k,p}_L(Q)&lt;/script&gt;を以下の性質を有する集合とします．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;f \in C^{k,p}_L(Q)&lt;/script&gt; はk回微分可能で連続&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;f^{(p)}&lt;/script&gt;を&lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;のp階微分とすると，
&lt;script type=&quot;math/tex&quot;&gt;\|f^{(p)}(\boldsymbol{x}) - f^{(p)}(\boldsymbol{y})\|_2 \leq L\|\boldsymbol{x}-\boldsymbol{y}\|_2 \ \forall \boldsymbol{x},\boldsymbol{y} \in Q&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;この時&lt;script type=&quot;math/tex&quot;&gt;f\in C^{k,p}_L(Q)&lt;/script&gt;はリプシッツ連続であるという．&lt;/p&gt;

&lt;p&gt;今，目的関数が&lt;script type=&quot;math/tex&quot;&gt;f \in C^{1,1}_L(\mathbb{R}^d)&lt;/script&gt;だとしましょう．この時，以下が成立します．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
|f(\boldsymbol{y})-f(\boldsymbol{x})-\langle f&#39;(\boldsymbol{x}),\boldsymbol{y}-\boldsymbol{x} \rangle| \leq \frac{L}{2}\|\boldsymbol{y}-\boldsymbol{x}\|_2^2 \ \cdots (1)
\end{align*}&lt;/script&gt;

&lt;p&gt;さて，再び勾配法による一回分の更新を考えましょう．
&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{y}=\boldsymbol{x}-h\nabla f(\boldsymbol{x})&lt;/script&gt;とすると，(1)より，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
f(\boldsymbol{y}) &amp;\leq f(\boldsymbol{x}) + \langle \nabla f(\boldsymbol{x}), \boldsymbol{y}-\boldsymbol{x} \rangle + \frac{L}{2}\|\boldsymbol{y}-\boldsymbol{x}\|^2_2 \\
&amp;= f(\boldsymbol{x}) - h\left(1 - \frac{h}{2}L\right)\|\nabla f(\boldsymbol{x})\|^2_2
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;したがって，&lt;script type=&quot;math/tex&quot;&gt;h\left(1 - \frac{h}{2}L\right) \geq 0&lt;/script&gt;，つまり，&lt;script type=&quot;math/tex&quot;&gt;h \leq \frac{2}{L}&lt;/script&gt;であれば，&lt;script type=&quot;math/tex&quot;&gt;f(\boldsymbol{y}) \leq f(\boldsymbol{x})&lt;/script&gt;が言えます．
つまり，当たり前かもしれませんが，勾配法によって関数が単調に減少するかはステップサイズの決め方に依存するということです．
ここから，ステップサイズの決め方がいかに重要かがわかりますね．&lt;/p&gt;

&lt;p&gt;問題なのは&lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt;がわからないということです．
もっともナイーブなステップサイズの決定方法は&lt;script type=&quot;math/tex&quot;&gt;h_k = h \ \forall k=0,1,\ldots&lt;/script&gt;として，適当に&lt;script type=&quot;math/tex&quot;&gt;h&gt;0&lt;/script&gt;を決めてやることですが，これだと必ずしも&lt;script type=&quot;math/tex&quot;&gt;h \leq \frac{2}{L}&lt;/script&gt;となっている保証はありません．そこで，なんらかの工夫が必要です．&lt;/p&gt;

&lt;p&gt;基本的には，更新の際に，&lt;script type=&quot;math/tex&quot;&gt;f(\boldsymbol{y}) \leq f(\boldsymbol{x})&lt;/script&gt;となる&lt;script type=&quot;math/tex&quot;&gt;h&gt;0&lt;/script&gt;を選べば良いわけですが，どうやったら良いのでしょうか？
適当に&lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt;を決めましょう．この時，&lt;script type=&quot;math/tex&quot;&gt;f(\boldsymbol{y}) &gt; f(\boldsymbol{x})&lt;/script&gt;となってしまったとします．
この時の戦略は，&lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt;を大きくするか，小さくするかです．どうしたらいいでしょう？&lt;/p&gt;

&lt;p&gt;もしも，更新によって関数値が増加してしまう時，つまり&lt;script type=&quot;math/tex&quot;&gt;f(\boldsymbol{y})&gt;f(\boldsymbol{x})&lt;/script&gt;の時，ステップサイズが&lt;script type=&quot;math/tex&quot;&gt;h&gt;\frac{2}{L}&lt;/script&gt;となっていることがわかります．ここから，&lt;script type=&quot;math/tex&quot;&gt;f(\boldsymbol{y}) \leq f(\boldsymbol{x})&lt;/script&gt;となるまで&lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt;を小さくしていけば良いことがわかります．
これは，更新によって，局所解または最適解を通り過ぎてしまったので，通り過ぎない程度に引き戻すことに相当します．
こんな感じでステップサイズを決めれば，一応目的関数は単調に減少していきます．&lt;/p&gt;

&lt;p&gt;まとめると，勾配法を使う際に最低限考えなければならないことは，&lt;script type=&quot;math/tex&quot;&gt;f(\boldsymbol{x}_{k+1}) \leq f(\boldsymbol{x}_k)&lt;/script&gt;となるように更新することで，これはステップサイズの決定に委ねられる．ステップサイズは，&lt;script type=&quot;math/tex&quot;&gt;f(\boldsymbol{x}_{k+1}) \leq f(\boldsymbol{x}_k)&lt;/script&gt;を満たすように決めれば良いが，もしこれが満たされない場合はステップサイズを小さくしていけば良い．&lt;/p&gt;

&lt;p&gt;こんな感じですかね．実際は，ステップサイズの決め方にもGoldstein-Armijo ruleとか，いろいろあるのでそれを使えばいいんですけどね．&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Jan 2016 06:20:07 +0900</pubDate>
        <link>http://nktmemoja.github.io/jekyll/update/2016/01/11/gradient-method.html</link>
        <guid isPermaLink="true">http://nktmemoja.github.io/jekyll/update/2016/01/11/gradient-method.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>ハードマージンSupport Vector Machine (SVM)の定式化</title>
        <description>&lt;p&gt;&lt;a rel=&quot;nofollow&quot; href=&quot;http://www.amazon.co.jp/gp/product/4061529064/ref=as_li_ss_tl?ie=UTF8&amp;amp;camp=247&amp;amp;creative=7399&amp;amp;creativeASIN=4061529064&amp;amp;linkCode=as2&amp;amp;tag=nettodesyuu00-22&quot; target=&quot;_blank&quot;&gt;サポートベクトルマシン (機械学習プロフェッショナルシリーズ)&lt;/a&gt;&lt;img src=&quot;http://ir-jp.amazon-adsystem.com/e/ir?t=nettodesyuu00-22&amp;amp;l=as2&amp;amp;o=9&amp;amp;a=4061529064&quot; width=&quot;1&quot; height=&quot;1&quot; border=&quot;0&quot; alt=&quot;&quot; style=&quot;border:none !important; margin:0px !important;&quot; /&gt;
を読んでいます．
SVMについて，わかっている気になっていましたが，わかっていませんでした．
意外と奥が深いですね．というわけでその整理の意味も込めてのポスト．&lt;/p&gt;

&lt;p&gt;この記事ではハードマージンSVMを考えます．
ハードマージンSVMでは，与えられたデータは線形分離可能であると仮定します．
つまり，超平面&lt;script type=&quot;math/tex&quot;&gt;f(\boldsymbol{x})=\boldsymbol{w}^T \boldsymbol{x} + b&lt;/script&gt;で，与えられた訓練データがミスなく分離できることを仮定します．
このような超平面は複数存在すると考えられますが，
SVMでは，「マージンを最大化する超平面を求める」，というのはよくある説明ですね．&lt;/p&gt;

&lt;p&gt;ではまず，マージンの定義から入りましょう．
今，訓練データ&lt;script type=&quot;math/tex&quot;&gt;\{(\boldsymbol{x}_i,y_i)\}_{i=1}^n, \boldsymbol{x}_i \in \mathbb{R}^d, y_i \in \{-1,1\} \ \forall i=1, \ldots, n&lt;/script&gt;が与えられているとします．
マージンを，
超平面&lt;script type=&quot;math/tex&quot;&gt;f(\boldsymbol{x})=\boldsymbol{w}^T \boldsymbol{x} + b&lt;/script&gt;からもっとも近いサンプルまでの距離と定義します．
つまり，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
i^{*} = \mathop{\rm arg\,min}\limits_{1 \leq i \leq n} \ \frac{|\boldsymbol{w}^T \boldsymbol{x}_i + b|}{\|\boldsymbol{w}\|}
= \mathop{\rm arg\,min}\limits_{1 \leq i \leq n} \ |\boldsymbol{w}^T \boldsymbol{x}_i + b|
\end{align*}&lt;/script&gt;

&lt;p&gt;と定義すると，マージンは&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\frac{|\boldsymbol{w}^T \boldsymbol{x}_{i^{*}} + b|}{\|\boldsymbol{w}\|}
\end{align*}&lt;/script&gt;

&lt;p&gt;と表すことができます．
全ての訓練データを正しく分類しつつ，このマージンを最大化したいので，以下の最適化問題を解きます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\max_{\boldsymbol{w},b} &amp;\ \frac{|\boldsymbol{w}^T \boldsymbol{x}_{i^{*}} + b|}{\|\boldsymbol{w}\|} \\
s.t. &amp;\ y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b) &gt; 0 \ \forall i=1,\ldots ,n
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;分子が邪魔なので，これを以下のように書き換えます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\max_{\boldsymbol{w},b} &amp;\ \frac{1}{\|\boldsymbol{w}\|} \\
s.t. &amp;\ y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b) &gt; 0 \ \forall i=1,\ldots ,n \\
&amp;\ |\boldsymbol{w}^T \boldsymbol{x}_{i^{*}} + b |=1
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;二つの制約条件を一つにまとめると，以下のようになります．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\max_{\boldsymbol{w},b} &amp;\ \frac{1}{\|\boldsymbol{w}\|} \\
s.t. &amp;\ y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b) \geq 1 \ \forall i=1,\ldots ,n \\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;個人的にはここの変形が一番きつかったですね…．
なので軽く補足しときます．
以下の制約を書き換えていきます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
 y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b) &gt; 0 \ \forall i=1,\ldots ,n \ \cdots (1) \\
 |\boldsymbol{w}^T \boldsymbol{x}_{i^{*}} + b |=1 \ \cdots (2)
\end{align}&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;i^{*}&lt;/script&gt;の定義から，(2)は以下のように書き換えられます．&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\ |\boldsymbol{w}^T \boldsymbol{x}_{i} + b| \geq 1  \ \forall i=1,\ldots ,n \ \cdots (2&#39;)
\end{align}&lt;/script&gt;

&lt;p&gt;ここで，(1)と(2’)を合体させたいのですが，
&lt;script type=&quot;math/tex&quot;&gt;y_i(\boldsymbol{w}^T \boldsymbol{x}_{i} + b) = |\boldsymbol{w}^T \boldsymbol{x}_{i} + b|&lt;/script&gt;
を示すことができれば良さそうです．
ここで，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b) = \left\{
\begin{array}
\boldsymbol{w}^T \boldsymbol{x}_i + b &amp; (y_i=1) \\
-(\boldsymbol{w}^T \boldsymbol{x}_i + b) &amp; (y_i=-1)
\end{array}
\right.
\end{align*}, %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
|\boldsymbol{w}^T \boldsymbol{x}_i + b| = \left\{
\begin{array}
\boldsymbol{w}^T \boldsymbol{x}_i + b &amp; (\boldsymbol{w}^T \boldsymbol{x}_i + b \geq 0) \\
-(\boldsymbol{w}^T \boldsymbol{x}_i + b) &amp; (\boldsymbol{w}^T \boldsymbol{x}_i + b &lt; 0)
\end{array}
\right.
\end{align*}, %]]&gt;&lt;/script&gt;

&lt;p&gt;ですが，(1)より，&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}^T \boldsymbol{x}_i + b \geq 0&lt;/script&gt; は成り立たず，この文脈では必ず
&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}^T \boldsymbol{x}_i + b &gt; 0&lt;/script&gt;となります．&lt;/p&gt;

&lt;p&gt;なので，&lt;script type=&quot;math/tex&quot;&gt;y_i=1 \Leftrightarrow \boldsymbol{w}^T \boldsymbol{x}_i + b &gt; 0&lt;/script&gt;と
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
y_i=-1 \Leftrightarrow \boldsymbol{w}^T \boldsymbol{x}_i + b &lt; 0 %]]&gt;&lt;/script&gt;を示すことにします．
すごく簡単すぎて示す必要もないかもしれませんが…．&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;y_i=1&lt;/script&gt;の時，(1)より，&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}^T \boldsymbol{x}_i + b &gt; 0&lt;/script&gt;．
&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}^T \boldsymbol{x}_i + b &gt; 0&lt;/script&gt;の時，(1)と&lt;script type=&quot;math/tex&quot;&gt;y_i \in \{-1,1\}&lt;/script&gt;より，&lt;script type=&quot;math/tex&quot;&gt;y_i=1&lt;/script&gt;．
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
y_i=-1 \Leftrightarrow \boldsymbol{w}^T \boldsymbol{x}_i + b &lt; 0 %]]&gt;&lt;/script&gt;も同様．&lt;/p&gt;

&lt;p&gt;よって，
&lt;script type=&quot;math/tex&quot;&gt;y_i(\boldsymbol{w}^T \boldsymbol{x}_{i} + b) = |\boldsymbol{w}^T \boldsymbol{x}_{i} + b|&lt;/script&gt;．&lt;/p&gt;

&lt;p&gt;そういうわけで，(2’)は，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\ y_i(\boldsymbol{w}^T \boldsymbol{x}_{i} + b) \geq 1  \ \forall i=1,\ldots ,n \ \cdots (2&#39;&#39;)
\end{align}&lt;/script&gt;

&lt;p&gt;これで，(1)と(2’‘)を合体できますね．
最後に最大化と最小化を書き換えて，ノルムを2乗すれば，よく見るハードマージンSVMの最適化問題の完成です：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\min_{\boldsymbol{w},b} &amp;\ \|\boldsymbol{w}\|^2 \\
s.t. &amp;\ y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b) \geq 1 \ \forall i=1,\ldots ,n 
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;ここで，与えられたデータに対して，制約&lt;script type=&quot;math/tex&quot;&gt;y_i(\boldsymbol{w}^T \boldsymbol{x}_i + b) \geq 1 \ \forall i=1,\ldots ,n&lt;/script&gt;を満たす&lt;script type=&quot;math/tex&quot;&gt;\boldsymbol{w}&lt;/script&gt;と&lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt;が存在しないかもしれません．
というより，現実問題ではそのような場合がほとんどだと考えられます．
そこで，ソフトマージンの考え方が出てきます．
これについてはまた後日書けたらと思います．&lt;/p&gt;
</description>
        <pubDate>Fri, 08 Jan 2016 00:30:29 +0900</pubDate>
        <link>http://nktmemoja.github.io/jekyll/update/2016/01/08/hardmarginsvmformulation.html</link>
        <guid isPermaLink="true">http://nktmemoja.github.io/jekyll/update/2016/01/08/hardmarginsvmformulation.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>正例とラベル無しデータからの学習 (PU classification)</title>
        <description>&lt;p&gt;通常の2値分類問題では，正例と負例が与えられています． しかし扱う問題によっては，このようなデータを用意するのが困難な時があります． 例えば，抽出型のタスクです． 抽出型のタスクでは，抽出したい対象を正例と考えます． この場合の負例は「正例以外のデータ」と定義するほかありません． しかし，集めた正例に対し，それ以外のデータを負例と定義してしまうと， それ以外のデータに含まれる正例も負例として扱ってしまいます．&lt;/p&gt;

&lt;p&gt;このように負例を定義するのが難しい場合には，正例とラベルなしデータから学習する枠組み，PU classificationが有用です． PU classificationについては，Elkan and Noto 2008を参照していただければと思うのですが，少しだけ解説しておきます． 3つの確率変数&lt;script type=&quot;math/tex&quot;&gt;x,y,s&lt;/script&gt;を考えます．ここで，&lt;script type=&quot;math/tex&quot;&gt;x \in \mathbb{R}, y \in \{-1,1\}, s \in \{0,1\}&lt;/script&gt;だとします． &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;は入力，&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;はクラスラベル，そして&lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;はデータがラベリングされるかどうかを表しています． 我々が欲しいのは，
&lt;script type=&quot;math/tex&quot;&gt;p(y|x)&lt;/script&gt;
ですが，PU classificationでは&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;は観測することができません． 我々のゴールは，&lt;script type=&quot;math/tex&quot;&gt;\{(x_i,s_i)\}_{i=1}^n&lt;/script&gt;から
&lt;script type=&quot;math/tex&quot;&gt;p(y|x)&lt;/script&gt;
を学習することです． 結果からいうと，2つの仮定をおくことで，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y=1|x) = \frac{p(s=1|x)}{p(s=1|y=1)}&lt;/script&gt;

&lt;p&gt;と表せます．
&lt;script type=&quot;math/tex&quot;&gt;p(s=1|x)&lt;/script&gt;
は与えられたデータから推定できます． そして，
&lt;script type=&quot;math/tex&quot;&gt;p(s=1|y=1)&lt;/script&gt;
は開発データから推定できます． 詳しくはElkan and Noto 2008の2章にまとめられています． 今回はElkan and Noto 2008の手法を用いてPU classificationを行っていきます．&lt;/p&gt;

&lt;p&gt;では，以下のような正解データを考えましょう． &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-true_labeled.png&quot; alt=&quot;true_labeled.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;このデータに対して，実際に与えられるのは以下のようなデータです． &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-pu_data.png&quot; alt=&quot;pu_data.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;このデータに対して，まずは通常のロジスティック回帰を適用してみます． なお，今回は負例が多いので，交差確認法には正例側のF値を用います． 結果は以下のようになりました． &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-result_of_tradclf1.png&quot; alt=&quot;result_of_tradclf.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ご覧のように，全て負例だと予測してしまいました． 次に，PU classificationを適用してみます． &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-result_of_puclassification.png&quot; alt=&quot;result_of_puclassification.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;正例とラベルなしデータから，うまく分類境界を学習できていることがわかります．&lt;/p&gt;

&lt;p&gt;デモ用のコードは以下に載せておきますのでぜひ試してみてください． ちなみに，非線形な分類境界を表現するための&lt;a href=&quot;https://gist.github.com/nkt1546789/e41199340f7a42c515be&quot; title=&quot;rbfmodel_wrapper.py&quot;&gt;rbfmodel_wrapper.py&lt;/a&gt; と PU Classificationのための&lt;a href=&quot;https://gist.github.com/nkt1546789/9fbbf2f450779bde60c3&quot; title=&quot;puwrapper.py&quot;&gt;puwrapper.py&lt;/a&gt; も合わせてDLしてください．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://gist.github.com/nkt1546789/e9421f06ea3a62bfbb8c&quot; title=&quot;https://gist.github.com/nkt1546789/e9421f06ea3a62bfbb8c&quot;&gt;https://gist.github.com/nkt1546789/e9421f06ea3a62bfbb8c&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Thu, 29 Oct 2015 21:47:00 +0900</pubDate>
        <link>http://nktmemoja.github.io/machine%20learning/2015/10/29/29214700.html</link>
        <guid isPermaLink="true">http://nktmemoja.github.io/machine%20learning/2015/10/29/29214700.html</guid>
        
        <category>Machine Learning</category>
        
        <category>PU classification</category>
        
        
        <category>Machine Learning</category>
        
      </item>
    
      <item>
        <title>Word2Vecを使った単語間関係分類</title>
        <description>&lt;p&gt;単語間には様々な関係があります． 今回は，単語間の関係をWord2Vecで学習させようと思います． Word2Vecにはアナロジーを捉えられるという話があります． あの有名な，king - man + woman = queen というやつですね． これは，king - man = queen - womanとも書けます． つまり，2語間の差が関係を表しており， この例だと，kingとmanの関係とqueenとwomanの関係が同じであると捉えることができます．&lt;/p&gt;

&lt;p&gt;さて，Word2Vecで学習したベクトル表現を使うと，差ベクトルがうまいこと関係を表すと書きましたが， 必ずしもそうなっているとは限りません． 加えて，ユーザが扱いたい関係とWord2Vecで学習した関係が一致しているとも限りません． そこで，今回も例のごとく教師あり学習を使います． ユーザは教師データを通して，扱いたい関係をアルゴリズムに伝えることができます．&lt;/p&gt;

&lt;p&gt;最初からあまり多くの関係を対象にするのはしんどいので，今回はis-a, has-a関係のみに着目します． これは，僕の理解では，柔道 is-a スポーツ，スポーツ has-a 柔道みたいなものだと思っています． まずは&lt;a href=&quot;https://gist.github.com/nkt1546789/a3b3a4c166fc1c0486a1&quot; title=&quot;training data&quot;&gt;training data&lt;/a&gt;を用意します． is-aとhas-aは反対の関係になっていると思うので，has-aのデータだけ用意します． この中には (スポーツ，野球)というhas-a関係を表す順序対がリストで格納されています． リスト内の順序対に対して差ベクトルを計算し，一つのデータとして扱います．&lt;/p&gt;

&lt;p&gt;コードは以下のようになりました．&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;training&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;gensim.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Word2Vec&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisticRegressionCV&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Word2Vec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/path/to/your/w2v_model&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;permutation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ntr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;itr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ntr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ite&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ntr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Xtr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;itr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ytr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;itr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Xte&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;yte&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LogisticRegressionCV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xtr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ytr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ypred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xte&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yte&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ypred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# &amp;amp;gt;&amp;amp;gt;&amp;amp;gt; 0.99502487562189057&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;今回は厳密な実験はせずに，簡単に性能を見てみました． テストデータに対して，99%の精度を出すことができました． 以下簡単なテストデータへの予測例です．&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ypred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;has-a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;is-a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# results:&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;ブルーベリー&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;果物&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;動物&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;モルモット&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;動物&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;ワタボウシタマリン&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;スポーツ&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;スポーツ&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;登山&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;スポーツ&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;ロデオ&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;スポーツ&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;動物&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;ユーラシアカワウソ&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;スポーツ&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;フリーダイビング&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;競馬&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;スポーツ&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;スポーツ&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;ゴルフ&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;いかがでしょうか？うまく狙った関係が分類できていると思います． とりあえずはうまくいきました！これで成功です！ これがどこまで実用に耐えられるかはやってみないとわかりませんが．&lt;/p&gt;

</description>
        <pubDate>Tue, 27 Oct 2015 20:57:00 +0900</pubDate>
        <link>http://nktmemoja.github.io/machine%20learning/2015/10/27/27205700.html</link>
        <guid isPermaLink="true">http://nktmemoja.github.io/machine%20learning/2015/10/27/27205700.html</guid>
        
        <category>Machine Learning</category>
        
        <category>NLP</category>
        
        <category>python</category>
        
        
        <category>Machine Learning</category>
        
      </item>
    
      <item>
        <title>Word2Vec+教師あり次元削減で文書分類+単語分類</title>
        <description>&lt;p&gt;前回:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://nktmemo.wordpress.com/2015/09/29/%E6%96%87%E6%9B%B8%E5%88%86%E9%A1%9E%E5%99%A8%E3%81%A7%E5%8D%98%E8%AA%9E%E5%88%86%E9%A1%9E%E3%82%92%E3%81%97%E3%81%A6%E3%81%BF%E3%82%8B/&quot; title=&quot;https://nktmemo.wordpress.com/2015/09/29/%E6%96%87%E6%9B%B8%E5%88%86%E9%A1%9E%E5%99%A8%E3%81%A7%E5%8D%98%E8%AA%9E%E5%88%86%E9%A1%9E%E3%82%92%E3%81%97%E3%81%A6%E3%81%BF%E3%82%8B/&quot;&gt;文書分類器で単語分類をしてみる&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;の続きです．&lt;/p&gt;

&lt;p&gt;まずは，前回のおさらい． 前回は，文書に対してはラベル付きデータが与えられており，単語についてはラベルなしデータが与えられているという設定を考えました． そして，文書と単語が同じ空間に存在すれば，半教師付き学習に帰着することを示しました． 詳しくは前回の記事を見ていただくとして，これからは半教師付き学習の設定で話を進めます．&lt;/p&gt;

&lt;p&gt;今回は，前回の記事でいう単語ベクトル集合&lt;script type=&quot;math/tex&quot;&gt;\{x_i\}_{i=n_d}^{n}&lt;/script&gt;をWord2Vecで学習させます． 食わせるコーパスは分類対象の文書集合です．&lt;/p&gt;

&lt;p&gt;その後，教師あり次元削減手法であるFisher Discriminant Analysis (FDA)を使って&lt;script type=&quot;math/tex&quot;&gt;B:\mathbb{R}^d\rightarrow\mathbb{R}^m&lt;/script&gt;を学習させます． これによって，Word2Vecで学習した単語ベクトルたちは，より低次元の空間に落とし込まれます． つまり，&lt;script type=&quot;math/tex&quot;&gt;z=Bx&lt;/script&gt;として，&lt;script type=&quot;math/tex&quot;&gt;\{(z_i,y_i)\}_{i=1}^{n_d}&lt;/script&gt;と&lt;script type=&quot;math/tex&quot;&gt;\{z_i\}_{i=n_d}^{n}&lt;/script&gt;を得ます． 今回は，&lt;script type=&quot;math/tex&quot;&gt;m=c-1&lt;/script&gt;としました，ただし，&lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt;はクラス数です．&lt;/p&gt;

&lt;p&gt;なぜこのような処理をするかというと，Word2Vecのような教師なし学習では，必ずしも「望ましい結果」が得られるとは限りません． なぜなら，「望ましい結果」についての情報を一切与えていないからです． 今回の目的は文書分類+単語分類です． この目的に対して，Word2Vecが必ずしも望ましい結果を返すとは限らないのです．&lt;/p&gt;

&lt;p&gt;そこで，教師あり次元削減を使います． FDAは，簡単にいうと，同じクラスに属するサンプルは近く，異なるクラスに属するサンプルは遠くなるよう，射影行列を学習します． ここでは，「望ましい結果」教師データとして与えるので，学習後の空間は分類という目的に対して望ましい空間になっていると期待できます．&lt;/p&gt;

&lt;p&gt;さて，あとは対して面白いことはしていません． &lt;script type=&quot;math/tex&quot;&gt;\{(z_i,y_i)\}_{i=1}^{n_d}&lt;/script&gt;で確率的分類器を学習させ，&lt;script type=&quot;math/tex&quot;&gt;\{z_i\}_{i=n_d}^{n}&lt;/script&gt;に対して予測をします．&lt;/p&gt;

&lt;p&gt;前回と同じデータを使って実験をしました． 結果の出力には同様にwordcloudを使わせてもらいました．&lt;/p&gt;

&lt;p&gt;ガールズ: &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-girls.png&quot; alt=&quot;girls.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ニュース・ゴシップ: &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-news1.png&quot; alt=&quot;news.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;エンタメ・カルチャー: &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-entertainment1.png&quot; alt=&quot;entertainment.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;おでかけ・グルメ: &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-spot1.png&quot; alt=&quot;spot.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;暮らし・アイデア: &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-life1.png&quot; alt=&quot;life.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;レシピ: &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-recipe1.png&quot; alt=&quot;recipe.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;カラダ: &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-wellness1.png&quot; alt=&quot;wellness.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ビジネススキル: &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-business1.png&quot; alt=&quot;business.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;IT・ガジェット: &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-tech1.png&quot; alt=&quot;tech.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;デザイン・アート: &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-design1.png&quot; alt=&quot;design.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;雑学: &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-trivia1.png&quot; alt=&quot;trivia.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;おもしろ: &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-humor1.png&quot; alt=&quot;humor.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;定番: &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/10/wpid-popular1.png&quot; alt=&quot;popular.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;評価データがなくて定性的に評価するしかないのですが，前回と比べて，かなり改善されている気がします． 雑学とかおもしろ，定番なんかは定義がよくわからなくて判断しにくいですが，それ以外はうまく単語分類ができていると思います．&lt;/p&gt;

&lt;p&gt;今回は，Word2Vec+教師あり次元削減 (FDA) を使って文書分類器を作成し，それを使って単語分類をしてみました． 結果として，このアプローチはなかなか良いと感じました． 文書分類，単語分類については，これでひと段落した感じがします． 本当は単語分類なんかはマルチラベル分類問題として解くべくなのかもしれませんが，あまりこの問題に執着してもあれなので． 次は要約や，トレンド抽出なんかをやっていきたいなあなんて思っています．&lt;/p&gt;

&lt;p&gt;前回と今回はコードを載せていません． これはコードがなかなか複雑なためです． もし，見てみたいという方がいたら，コメントからでも，Twitterからでもなんでも良いので言ってください！ 読んでいただき，ありがとうございました．&lt;/p&gt;

</description>
        <pubDate>Tue, 06 Oct 2015 20:46:00 +0900</pubDate>
        <link>http://nktmemoja.github.io/machine%20learning/2015/10/06/06204600.html</link>
        <guid isPermaLink="true">http://nktmemoja.github.io/machine%20learning/2015/10/06/06204600.html</guid>
        
        <category>Machine Learning</category>
        
        <category>NLP</category>
        
        <category>python</category>
        
        
        <category>Machine Learning</category>
        
      </item>
    
      <item>
        <title>文書分類器で単語分類をしてみる</title>
        <description>&lt;p&gt;keywords: 文書分類 (document classification）， 単語分類（word classification）， Pointwise mutual information&lt;/p&gt;

&lt;h2 id=&quot;sec-1&quot;&gt;はじめに&lt;/h2&gt;
&lt;p&gt;文書へのラベリングと単語へのラベリングはどちらが簡単だろう？ 例えば多くのニュースサイトではすでに文書は分類されている． しかし，単語が分類されているのは見たことがない． というより，そんなものを表に出してもあまり意味がないので表に出ていないのだろう． この状況を踏まえると，データをクロールする側からすると，ラベル付き文書データを入手するのは容易で， ラベル付き単語データを入手するのは困難だと言える．&lt;/p&gt;

&lt;p&gt;いま，文書データをクロールして，検索エンジンを作ることを考えよう． 各文書にはラベルが付いている． このラベル情報を活かせないか？ 例えばクエリにラベルが付いていれば，クエリと文書のラベルを見て，一致するものを出せばよい，あるいはそういう場合にスコアが高くなるように，検索エンジンのスコアを設計すれば良い． このように，単語へのラベリングはある程度需要があると推測される．&lt;/p&gt;

&lt;h2 id=&quot;sec-2&quot;&gt;定式化&lt;/h2&gt;

&lt;p&gt;さて，今回やるのは，ラベル付き文書データを使って，単語分類をしようというもの． つまり，持っているものは，&lt;script type=&quot;math/tex&quot;&gt;\{(d_i,y_i)\}_{i=1}^{n_d}&lt;/script&gt;と&lt;script type=&quot;math/tex&quot;&gt;\{w_i\}_{i=1}^{n_w}&lt;/script&gt;， ただし，&lt;script type=&quot;math/tex&quot;&gt;d_i \in \mathcal{D}&lt;/script&gt;は文書，&lt;script type=&quot;math/tex&quot;&gt;y_i \in \mathcal{Y}&lt;/script&gt;は文書に対するラベル, &lt;script type=&quot;math/tex&quot;&gt;w_i \in \mathcal{W}&lt;/script&gt;は単語を意味する．&lt;/p&gt;

&lt;p&gt;ここで，もし単語と文書が同じ空間に存在すれば，文書分類器を使って単語分類ができると思われる． つまり，&lt;script type=&quot;math/tex&quot;&gt;\mathcal{S}=\mathcal{D}=\mathcal{W}&lt;/script&gt;とし，なんらかの変換&lt;script type=&quot;math/tex&quot;&gt;\phi:\mathcal{S} \rightarrow \mathcal{X}&lt;/script&gt;を定義すればよい． ここまで来れば，&lt;script type=&quot;math/tex&quot;&gt;x_i=\phi(d_i) \ \forall i=1,\ldots,n_d, \ x_{n_w+j}=\phi(w_j) \ \forall j=1,\ldots,n_w&lt;/script&gt;とし， &lt;script type=&quot;math/tex&quot;&gt;\{(x_i,y_i)\}_{i=1}^{n_d}&lt;/script&gt;と&lt;script type=&quot;math/tex&quot;&gt;\{x_i\}_{i=n_d}^{n}&lt;/script&gt;を得る．ただし&lt;script type=&quot;math/tex&quot;&gt;n=n_d+n_w&lt;/script&gt;． こうして見てみると，単語と文書を同じ空間に写像すれば，これは半教師付き分類問題に帰着することがわかる．&lt;/p&gt;

&lt;p&gt;簡単のために，&lt;script type=&quot;math/tex&quot;&gt;\mathcal{X}=\mathbb{R}^d, \ \mathcal{Y}=\{1,\ldots,c\}&lt;/script&gt;とする． 今回は，「文書は単語の線形結合で表される」という仮定を置いてみる．つまり， &lt;script type=&quot;math/tex&quot;&gt;d=\sum_{i=1}^{n_w} \alpha^{(d)}_i w_i, \ \alpha^{(d)}_i \in \mathbb{R} \ \forall_i=1,\ldots,n_w&lt;/script&gt;  となる．さらに，「&lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;は線形写像である」という仮定を置くと，
 &lt;script type=&quot;math/tex&quot;&gt;\phi(d)=\sum_{i=1}^{n_w} \alpha^{(d)}_i \phi(w_i)&lt;/script&gt;
 となる．というわけで，&lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;ではなくて，コーパスから&lt;script type=&quot;math/tex&quot;&gt;\{\phi(w_i)\}_{i=1}^{n_w}&lt;/script&gt;を学習することにする．&lt;/p&gt;

&lt;p&gt;さて，やらなければならないのは，&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;コーパスから&lt;script type=&quot;math/tex&quot;&gt;\{\phi(w_i)\}_{i=1}^{n_w}&lt;/script&gt;を学習する&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;の決定&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;である．だいぶシンプルになったな．1に関しては死ぬほど研究されているので，その中から適用な手法を使うことにする． ここでは，PPMIを使って単語ベクトルを決定してみる．この辺は特に珍しくもないので，例えば以下を参照してください．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cl.ecei.tohoku.ac.jp/nlp100/&quot; title=&quot;http://www.cl.ecei.tohoku.ac.jp/nlp100/&quot;&gt;http://www.cl.ecei.tohoku.ac.jp/nlp100/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;残る問題は２だ．とりあえずシンプルさを追求して，単語の出現回数を使うことにする．つまり，&lt;script type=&quot;math/tex&quot;&gt;\alpha^{(d)}_i=c(w_i,d)&lt;/script&gt;とする． ただし，&lt;script type=&quot;math/tex&quot;&gt;c(w_i,d)&lt;/script&gt;は，文書&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;における単語&lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt;の出現回数である． これで全ての問題が一応解決した．さあ，あとは実装するだけ．&lt;/p&gt;

&lt;div id=&quot;outline-container-sec-3&quot; class=&quot;outline-2&quot;&gt;
&lt;h2 id=&quot;sec-3&quot;&gt;実装&lt;/h2&gt;
&lt;div class=&quot;outline-text-2&quot; id=&quot;text-3&quot;&gt;
 コードは後日載せます． やっていることは，PPMIを要素とした単語-文脈行列を作り，その各行を単語ベクトルとします． あとは↑の定式化通りに文書ベクトルを生成し，文書分類器を作ります． その後，単語ベクトルたちを分類器にかけます． 文書分類器には，ロジスティック回帰（sklearn.linear_model.LogisticRegressionCV）を用います． デフォルト設定です（アプローチの可能性を見たいだけなので）． 
&lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;sec-4&quot;&gt;実験&lt;/h2&gt;
&lt;p&gt;データはnaverまとめからクロールしたものを使う． カテゴリとそれに対応するクロールした文書数を以下の表に示す． これが今回の訓練データ．&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;カテゴリ&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;文書数&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ガールズ&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;600&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ニュース・ゴシップ&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;976&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;エンタメ・カルチャー&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;480&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;おでかけ・グルメ&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;867&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;暮らし・アイデア&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;737&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;レシピ&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;702&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;カラダ&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;708&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;ビジネススキル&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;558&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;IT・ガジェット&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;231&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;デザイン・アート&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;479&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;雑学&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;667&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;おもしろ&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;584&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;定番&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;257&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;総異なり語数は14767件で，これが今回の分類対象となる． さて，結果はただ単語を羅列してもおもしろくないので，wordcloudを使おうと思う． これについては以下を参考にしました，ありがとうございます．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://qiita.com/kenmatsu4/items/9b6ac74f831443d29074&quot; title=&quot;http://qiita.com/kenmatsu4/items/9b6ac74f831443d29074&quot;&gt;http://qiita.com/kenmatsu4/items/9b6ac74f831443d29074&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div id=&quot;outline-container-sec-4-1&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-4-1&quot;&gt;ガールズ&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-4-1&quot;&gt;
&lt;div class=&quot;figure&quot;&gt; &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/09/wpid-girls.png&quot; alt=&quot;girls.png&quot; /&gt;  &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-sec-4-2&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-4-2&quot;&gt;ニュース・ゴシップ&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-4-2&quot;&gt;
&lt;div class=&quot;figure&quot;&gt; &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/09/wpid-news.png&quot; alt=&quot;news.png&quot; /&gt;  &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-sec-4-3&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-4-3&quot;&gt;エンタメ・カルチャー&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-4-3&quot;&gt;
&lt;div class=&quot;figure&quot;&gt; &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/09/wpid-entertainment.png&quot; alt=&quot;entertainment.png&quot; /&gt;  &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-sec-4-4&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-4-4&quot;&gt;おでかけ・グルメ&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-4-4&quot;&gt;
&lt;div class=&quot;figure&quot;&gt; &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/09/wpid-spot.png&quot; alt=&quot;spot.png&quot; /&gt;  &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-sec-4-5&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-4-5&quot;&gt;暮らし・アイデア&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-4-5&quot;&gt;
&lt;div class=&quot;figure&quot;&gt; &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/09/wpid-life.png&quot; alt=&quot;life.png&quot; /&gt;  &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-sec-4-6&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-4-6&quot;&gt;レシピ&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-4-6&quot;&gt;
&lt;div class=&quot;figure&quot;&gt; &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/09/wpid-recipe.png&quot; alt=&quot;recipe.png&quot; /&gt;  &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-sec-4-7&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-4-7&quot;&gt;カラダ&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-4-7&quot;&gt;
&lt;div class=&quot;figure&quot;&gt; &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/09/wpid-wellness.png&quot; alt=&quot;wellness.png&quot; /&gt;  &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-sec-4-8&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-4-8&quot;&gt;ビジネススキル&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-4-8&quot;&gt;
&lt;div class=&quot;figure&quot;&gt; &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/09/wpid-business.png&quot; alt=&quot;business.png&quot; /&gt;  &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-sec-4-9&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-4-9&quot;&gt;IT・ガジェット&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-4-9&quot;&gt;
&lt;div class=&quot;figure&quot;&gt; &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/09/wpid-tech.png&quot; alt=&quot;tech.png&quot; /&gt;  &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-sec-4-10&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-4-10&quot;&gt;デザイン・アート&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-4-10&quot;&gt;
&lt;div class=&quot;figure&quot;&gt; &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/09/wpid-design.png&quot; alt=&quot;design.png&quot; /&gt;  &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-sec-4-11&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-4-11&quot;&gt;雑学&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-4-11&quot;&gt;
&lt;div class=&quot;figure&quot;&gt; &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/09/wpid-trivia.png&quot; alt=&quot;trivia.png&quot; /&gt;  &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-sec-4-12&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-4-12&quot;&gt;おもしろ&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-4-12&quot;&gt;
&lt;div class=&quot;figure&quot;&gt; &lt;img src=&quot;http://nktmemo.files.wordpress.com/2015/09/wpid-humor.png&quot; alt=&quot;humor.png&quot; /&gt;  &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;outline-container-sec-4-13&quot; class=&quot;outline-3&quot;&gt;
&lt;h3 id=&quot;sec-4-13&quot;&gt;定番&lt;/h3&gt;
&lt;div class=&quot;outline-text-3&quot; id=&quot;text-4-13&quot;&gt;
 該当単語なし 
&lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;sec-5&quot;&gt;おわりに&lt;/h2&gt;
&lt;p&gt;今回はラベル付き文書データから文書分類器を学習し，それを単語分類に使用してみた． 結果は定性的に測るしかないが，うまくいっているところはあるのでアプローチは悪くないのかなと思う． 定番に該当がないのは定番だからなのだろうか？笑 ただ，もっと分類器をチューニングしたほうが良い気がする． いまはただロジスティック回帰にぶん投げているだけなので．&lt;/p&gt;

&lt;p&gt;次回は，教師あり次元削減，具体的にはFisher Discriminant Analysis (FDA)をかけてみます． いまは生の単語-文脈行列を使っているので，情報をもっと圧縮させて次元を削減しようと思います． さらに，教師ありデータを使うことで，同じラベルを持つものは近くなり，異なるラベルを持つものは遠くなるよう次元削減後の空間を学習します（正確には射影行列）． まぁとりあえずいいんではなかろうか．&lt;/p&gt;

</description>
        <pubDate>Tue, 29 Sep 2015 20:33:00 +0900</pubDate>
        <link>http://nktmemoja.github.io/machine%20learning/2015/09/29/29203300.html</link>
        <guid isPermaLink="true">http://nktmemoja.github.io/machine%20learning/2015/09/29/29203300.html</guid>
        
        <category>Machine Learning</category>
        
        <category>python</category>
        
        
        <category>Machine Learning</category>
        
      </item>
    
  </channel>
</rss>
