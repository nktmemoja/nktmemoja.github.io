---
layout: post
title:  "半教師付きページランクを用いたウェブページ中の単語への重み付け"
date: 2016-07-07 23:39:59 +0900
categories: jekyll update
---

## はじめに
前回

- <a href="http://nktmemoja.github.io/jekyll/update/2016/07/06/laplaciannodeweighting.html" target="_blank">ラプラス正則化を用いた半教師付きページランク</a>

というのを紹介してみました．
今回はこれをウェブページ中の単語への重み付けに応用してみます．
このアルゴリズムは半教師付きですが，教師データが自動的に得られる場合があります．
その一例が，ウェブページです．
ウェブページは<meta>内にタイトルやディスクリプション，キーワードなどの重要な情報を持っています．
これにより，教師データがほぼ自動的に得られます．
ウェブページでなくとも，論文だと，タイトル・アブストラクトなどが得られますし，大体のテキストに適用できるのではないかと思います．

単語への重み付けを，以下のように，半教師付きページランクの二段階適用によって行います．

1. テキストへの重み付け
2. 単語への重み付け

なお，ここでいうテキストとは，DOMツリーの中のテキストノードが持つテキストを指します．

### テキストへの重み付け

ウェブページ$$d$$を順序対を用いて$$d = (w_1, \ldots, w_n)$$と表します（これはDOMツリーを深さ優先探索し，テキストノードだけ拾えば得られます）．
ただし，$$w_i$$はテキストで，これも順序対を用いて
$$w_i = (w_{i1},\ldots,w_{in_{i}})$$
と表します．ここで，$$V$$を$$d$$に出現する単語集合とし，$$w_{ij} \in V$$とします．
本節では，$$\{w_i\}_{i=1}^n$$への重み$$\{f_{i}\}_{i=1}^n$$を獲得することを目的とします．
ただし，$$f_i$$は$$w_i$$への重みです．

半教師付きページランクは，隣接行列$$\mathbf{A}$$ と，初期値$$\mathbf{u}$$を必要としますので，これを定義するだけで済みます．
まず，$$\mathbf{A}$$を，以下で定義されるテキスト間の類似度行列とします．

$$
A_{ij} = sim(w_i, w_j)
$$

テキスト間類似度関数$$sim$$はコサインでもジャッカードでもなんでもいいです．
次に，$$\mathbf{u}$$を，以下のように定義します．

$$
u_i =
sim(w_i, w_{title}) + sim(w_i, w_{desc})
$$

ただし，$$w_{title}$$は$$d$$のタイトル，$$w_{desc}$$はディスクリプションとします．
あるいはもう少し厳格に，以下のようにタイトルとディスクリプションだけ使うことも考えられます．

$$
u_i =
\left\{\begin{array}{ll}
1 & if \ w_i = w_{title} \ or \ w_i = w_{desc} \\
0 & otherwise
\end{array}\right.
$$


これで半教師付きページランクを適用するのに必要なものが揃いました．
あとは実行して，$$\mathbf{f} = [f_1 \cdots f_n]$$を得ます．

ここでやっていることは，まず，$$\mathbf{u}$$でもって，タイトルとディスクリプションは大事だということを表現しています．
さらに，隣接行列をテキスト間の類似度行列とすることで，半教師付きページランクがタイトルとディスクリプションに似ているテキストは重要だと判断します．

### 単語への重み付け
上でテキストの重要度を計算した後，単語への重み付けを実行します．
本節の目的はウェブページ$$d$$中に出現する単語集合$$V=\{v_i\}_{i=1}^N$$への重み$$\{g_i\}_{i=1}^N$$を獲得することです．

ここでも半教師付きページランクを用います．
上と同じように隣接行列$$\mathbf{A}'$$を，

$$
A'_{ij} = sim'(v_i, v_j),
$$

と定義します．
$$\mathbf{u}'$$は，以下のように定義します．

$$
u'_i =
\left\{\begin{array}{ll}
1 & if \ v_i \in V_{title} \ or \ v_i \in V_{desc} \\
0 & otherwise
\end{array}\right.
$$

ただし，$$V_{title}$$はタイトル中に出現する単語集合，$$V_{desc}$$はディスクリプション中に出現する単語集合です．

単語間類似度関数$$sim'$$の定義についてですが，
今回は，単語の分布仮設の意味での類似度とします．
分布仮設は簡単にいうと，「共起する単語が似ている単語は意味的に似ている」と言うものです．
これは，例えば文脈ベクトルのコサイン類似度で定義されます．
これを行うために，$$C_{ij}$$を，$$v_i$$と$$v_j$$が共起範囲$$k$$で共起した回数として，共起行列$$\mathbf{C} = [\mathbf{c}_1 \cdots \mathbf{c}_N]$$を作ります．
$$v_i$$の文脈ベクトルは$$\mathbf{c}_i$$に対応します．
ここで，$$sim'$$を以下のように定義します．

$$
sim'(v_i, v_j) = \frac{\langle \mathbf{c}_i, \mathbf{c}_j \rangle}{\|\mathbf{c}_i\|\|\mathbf{c}_j\|}
$$

以上の定義で半教師付きページランクを適用することにより，単語への重み$$\mathbf{g} = [g_1 \cdots g_N] $$が得られます．

## 実験
実験してみました．
今回は結果をtf (term frequency)と比較します．
本当はtfidfと比較したいのですが，idfがコーパス依存なので，tfだけと比較します．

データとしては，比較的ノイズが多いページを選びました．
ノイズというのは，ヘッダーフッター，メニュー，広告などを指します．
最近のウェブページはこのような多くのノイズを含んでいます．
最近では，広告配信業者がページの内容に応じて関連の高い広告を埋め込んでいたりするので，ウェブページの解析がより難しくなっているように思います...．

話がそれましたが，以下の二つのURLをデータとします．

1. <a href="http://www.lifehacker.jp/2016/07/160702_grilledapple.html" target="_blank">屋外料理のデザートに、スモーキーで甘い「りんごのグリル」はいかが？</a>
2. <a href="http://tabelog.com/shizuoka/A2201/A220102/22000892/dtlrvwlst/37192867/?lid=unpickup_review" target="_blank">『生しらす！』by 0141 : 和食処 するが蕎 - 蒲原/そば [食べログ]</a>
3. <a href="http://curazy.com/archives/142844" target="_blank">癒されたい人集合！一生離れないと誓った「にこいちアニマル」に悶える12選 | CuRAZY [クレイジー]</a>

1, 2, 3に対する結果をそれぞれ表1, 2, 3に示します．
なお，タイトルに出現する単語は表示していません．
TFより明らかに良い結果となっていると思います．
ただ，ページ3のように画像メインのページだと情報が少なくてほとんどの要素が0になってしまいます．
これは，外部情報，例えばword2vecなどを用いて$$sim'$$を定義してやるといいかもしれません．

今回は可能性をみたかっただけなので，定性評価してしませんが，定量評価も比較的容易にできると思います．
というのも，この手法はよくあるDOM構造を利用したメインコンテンツ抽出とは違って，DOM構造に非依存なので，
同じドメインのページを大量に集めてテストしても大丈夫だと思います．
そういう意味ではパラメータもチューニングも自動化できるかもしれません．

次回は，word2vecなどを使って，性能改善を図りたいと思います．

|表1|提案法||TF||
|順位|単語|スコア|単語|スコア|
|:-:|:-|:-|
|1|サイド|0.028785|           title|0.01176|
|2|紹介|0.027523|			 トラブル|0.00941|
|3|food|0.021117|			 エアコン|0.00941|
|4|bbq|0.020636|			 男女|0.00941|
|5|肉|0.019335|				 温度|0.00941|
|6|ディッシュ|0.018379|		 設定|0.00941|
|7|メニュー|0.018254|		 staff|0.00706|
|8|考え|0.018254|			 by|0.00706|
|9|即座|0.017824|			 about|0.00706|
|10|フルーツ|0.013205|		 advertising|0.00706|
|11|メリット|0.010944|		 lifehacker|0.00706|
|12|概念|0.010802|			 privacy|0.00706|
|13|アイデア|0.010072|		 inc|0.00706|
|14|グラニースミス|0.006758| 生活|0.00706|
|15|酸味|0.006684|			 mediagene|0.00706|
|16|最適|0.006622|			 サイド|0.00706|
|17|リンク|0.006463|		 方法|0.00706|
|18|参照|0.006408|			 hot|0.00471|
|19|ベーコン|0.006405|		 円満|0.00471|
|20|フレーバー|0.00639|      旅行|0.00471|

|表2|提案法||TF||
|順位|単語|スコア|単語|スコア|
|:-:|:-|:-|
|1|店舗|0.000829|       口コミ|0.0344234079174|
|2|清水|0.000766|		名|0.0249569707401|
|3|蕎麦|0.00076|		店|0.0240963855422|
|4|天ぷら|0.000521|		件|0.0163511187608|
|5|うどん|0.000492|		点数|0.013769363167|
|6|情報|0.000431|		位|0.012908777969|
|7|茶漬け|0.000357|		店舗|0.0111876075731|
|8|揚げ物|0.000197|		情報|0.0103270223752|
|9|定食|0.000157|		ランキング|0.0103270223752|
|10|食堂|0.000157|		蕎麦|0.00946643717728|
|11|郷土料理|0.000155|	定食|0.00860585197935|
|12|料理|0.000122|		清水|0.00860585197935|
|13|魚介|0.000122|		クーポン|0.00860585197935|
|14|丼|0.00011|			訪問|0.00774526678141|
|15|重|8.1e-05|			食堂|0.00774526678141|
|16|海鮮料理|5.5e-05|	天ぷら|0.00774526678141|
|17|桜えび|4.4e-05|		桜えび|0.00774526678141|
|18|詳細|4.3e-05|		料理|0.00774526678141|
|19|青柳|4.3e-05|		特集|0.00774526678141|
|20|穴子|4.2e-05|      	無料|0.00688468158348|

|表3|提案法||TF||
|順位|単語|スコア|単語|スコア|
|1|animal|0.023053|     com|0.03412|
|2|tumblr.|0.015184|	post|0.0315|
|3|catasters|0.014318|	imgur|0.02887|
|4|thank|0.014265|		view|0.02887|
|5|you|0.012764|		on|0.02887|
|6|自然|0.005529|		24時|0.01837|
|7|笑顔|0.005529|		インターネット|0.01837|
|8|for|0.005331|		ネコ|0.01837|
|9|reminding|0.004836|	大好き!|0.01575|
|10|me|0.004675|		twitter|0.01575|
|11|to|0.004503|		記事|0.01575|
|12|wish|0.00446|		crazy|0.01312|
|13|a-ha|0.004407|		匹|0.0105|
|14|ppy|0.003951|		つぶやき|0.0105|
|15|記事|0.0|			一緒|0.0105|
|16|メニュー|0.0|		動画|0.00787|
|17|シェア|0.0|			シェア|0.00787|
|18|ネコ|0.0|			http://|0.00787|
|19|アプリ|0.0|			犬|0.00787|
|20|動物|0.0|			フォロー|0.00787|


<!--
このアルゴリズムにおける教師データは高い重みがついて欲しいノード集合なので，単語への重み付けの場合**高い重みがついて欲しい単語集合**になります．
これを$V^{*}$とします．

ウェブページはヘッダーフッター，メニュー，広告など多くのノイズを含んでいます．
最近では，広告配信業者がページの内容に応じて関連の高い広告を埋め込んでいたりするので，ウェブページの解析がより難しくなっているように思います．
しかしながら，**どんなにノイズが多くても，タイトル（とディスクリプション）は信頼できる**と考えられます．
というわけで，今回はタイトル（とディスクリプション）に含まれる単語集合を$V^{*}$$とします．
もちろんウェブページだけでなく，一般の文書にはタイトルがあると思うので，大体適用できると思います．

LNWを実行するには，$$V^{*}$$の他に，隣接行列$$\mathbf{A}$$が必要です．
今回は，これを共起行列とします．**重要な単語と共起する単語は重要**といった具合です．
これは，PageRankをテキストに応用したTextRankなんかと同じですね．
まとめると，

- $$V^{*}$$: タイトル（とディスクリプション）に出現する単語集合
- $$\mathbf{A}$$: 共起行列

とします．

# 実験
定性評価してみます．
対象は，

1. メインコンテンツがテキストのページ (<a href="http://gigazine.net/news/20160706-santa-susana-nuclear-disaster/" target="_blank">http://gigazine.net/news/20160706-santa-susana-nuclear-disaster/</a>)
2. そうでないページ (

1は例えばニュース記事などです．
2は例えば商品紹介ページだったり，画像や動画メインのページです．


|順位| 単語 |スコア|
|:---|:-----------------:|:-----------|
1 | アメリカ政府 |0.0108775415988|
2 | 徹底的 |0.0103866765743|
3 | 回避 |0.0102489698956|
4 | 放射性物質 |0.00826735437213|
5 | 表 |0.00688558477851|
6 | カリフォルニア州 |0.00687599731351|
7 | 完成 |0.00666602490429|
8 | 地球外生命 |0.00666602490429|
9 | 完全 |0.00662327332437|
10| 探査 |0.00654949037057|
11| fast |0.00654949037057|
12| 電波望遠鏡 |0.00654949037057|
13| 世界 |0.00654949037057|
14| 1979年 |0.00639230509713|
15| 危険 |0.00608159886266|
16| 例 |0.00597026076084|
17| 運用 |0.00586509783247|
18| カリフォルニア大学 |0.00578271053574|
19| 時点| 0.00554026259777|
20| アクシデント| 0.00532353224794|

図1: Gigazineへの適用結果<br/>



# おわりに
今回は，グラフノードへの重み付けアルゴリズムをウェブページに応用してみました．
ウェブページの解析といったら，メインコンテンツ抽出がよく研究されています．
画像や動画メインのページだと，メインとなるDOM要素を特定して，その中にある画像や動画を抽出する必要があります．
今回の応用例は，DOM要素の単語に重みをつけて，その合計値などでスコアをつければメインコンテンツ抽出にも使えると思います．
ちなみに，今回のアルゴリズムはDOM構造に依存していないので，同一ドメイン内で，評価やパラメータチューニングが簡単に行えることも強みだと思います．
-->
